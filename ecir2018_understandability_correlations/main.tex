
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{todonotes}

\usepackage{url}
\urldef{\mailsa}\path|{alfred.hofmann, ursula.barth, ingrid.haas, frank.holzwarth,|
\urldef{\mailsb}\path|anna.kramer, leonie.kunz, christine.reiss, nicole.sator,|
\urldef{\mailsc}\path|erika.siebert-cole, peter.strasser, lncs}@springer.com|    
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

%\usepackage[table]{xcolor}

%\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\mytodo}[1]{\textcolor{red}{#1}}

\begin{document}

\mainmatter  % start of an individual contribution

\title{Understanding Document Understandability Through Correlation Analysis}
%\titlerunning{Running Title}

% a short form should be given in case it is too long for the running head
%\titlerunning{Lecture Notes in Computer Science: Authors' Instructions}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Anonymous}
%
%\authorrunning{Lecture Notes in Computer Science: Authors' Instructions}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
%\institute{Springer-Verlag, Computer Science Editorial,\\
%Tiergartenstr. 17, 69121 Heidelberg, Germany\\
%\mailsa\\
%\mailsb\\
%\mailsc\\
%\url{http://www.springer.com/lncs}}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}
\maketitle


\section{Introduction}
\label{chp:understanding_understandability}

An existing concern of communicators is knowing if their message is well understood by their target public.  
Writing for a group of readers other than one's own is difficult.
This fact motived research over the past century on the development of readability formulas which can, through a single number, provide hints on the difficulty of a text.
Readability formulas were then widely adopted by different groups of the society, showing their effectiveness when increasing the amount learnt by recruits in the army \cite{klare55} or the readership of newspaper \cite{perry54}.
%became popular and are widely used. For example, different news outlets have are targeting different audiences: TV Guide....NewYorkTimes....
%....being implemented, for instance, in the Microsoft office package\footnote{\url{https://support.office.com/en-us/article/Test-your-document-s-readability-85b4969e-e80a-4777-8dd3-f7fc3c8b3fd2#ID0EAADAAA=2016,_2013,_2010}}.

Nevertheless, with advent and popularity of Web, in which anyone can write about anything, content has been largely generated without proper concern with its understandability.
Again: writing for a group of readers other than one's own is difficult and in some domains this might be dangerous.
The medical domain is one of such \cite{}.
The problem, as reported by X, is that health consumers might put themselves at risk if they misunderstand the content of what they read on the Web \cite{}.
Typically health consumers, such as patients and their next-of-kin, start their searches through commercial search engines \cite{}.
Thus, researchers in various areas of medicine have assessed the understandability of the material retrieved by popular search engines, often finding that they are harder to understand than they should be (e.g., \cite{graber99readability,fitzsimmons2010readability,wiener2013readability,patel13readability,atcherson14readability,meillier17readability}).
Mostly, this kind of research also relies on the output of readability formulas to deem if a Web page is easy or hard to understand.

However, as recently discussed in Palotti et al.\cite{palotti15}, the automated use of readability formulas firstly requires the content extraction from Web documents.
Palotti identified that the decision of appending a period at the end of each element in a list or table extracted from the HTML might result either in a single very long or many very short sentences, 
which drastically affects the interpretation of readability formula results. The same readability formula may yield results that vary from \textit{suitable even for kids} to \textit{understandable only if you are an experts} with only adding or removing a single period, a 'minor' implementation decision when cleaning HTML pages. 
One limitation of Palotti's work is not evaluating if either of these interpretations is correct.

In this paper, we propose a vast investigation on the preprocessing of Web documents and the correlation of their understandability estimation with human assessments.
For that, we take advantage of the understandability assessments made in recent Information Retrieval campaigns in CLEF eHealth (\cite{clef15,clef16}).

%\todo{Should we detail the tasks?} 
During the CLEF eHealth 2015 and 2016 campaigns \cite{clef15,clef16}, organizers explored Internet communities, such as Reddit AskDocs\footnote{\url{https://www.reddit.com/r/AskDocs/}} - in which people seek medical advice free of charge, to generate health consumer searches. Participants in CLEF eHealth campaigns were then instruct to search a considerable portion of the Web\footnote{In 2016 campaing ClueWeb-12 B was used: \url{http://lemurproject.org/clueweb12/}} for documents that could be topically relevant for each consumer query.
In addition to topical relevance, medical students and health professionals serving as assessors were instructed to assess how easy to understand and how much they would trust the information contained in each assessed document. % The assessment tool used in these campaigns is shown in Figure~\ref{fig:relevation} to enhance the reader understanding.
%
In this work, we extensively use the understandability assessments collected in these campaigns.
%We limit our analysis to the relevant documents to reduce the noise associated with understandability assessments, as the use of ClueWeb-12 B in 2016 campaign resulted in documents from a broad range of topics being retrieved, not only medical ones, and we would like to keep in our analysis only documents related to medicine.

%As also depicted in Figure~\ref{fig:relevation}, understandability assessments were given on a 4-label scale (scores from 0 to 3) in 2015 and on a 0-100 scale in 2016.
%Easy to understand documents had assessments closer to 0 while hard to understand document had their assessments close to the maximal value, 3 in 2015 and 100 in 2016.
%Altogether there are 1,452 documents assessed for understandability for CLEF eHealth 2015 and 3,320 for 2016 (see a detailed analysis in Appendix~\ref{apx:clef}, for example, Figures~\ref{fig:boxplot_labels} and~\ref{fig:boxplot_others} report the assessment distribution).

We correlate the human assessments with the output of various readability formulas applied with different preprocessing settings.
Our first intent is to complete Palotti's work, providing guidelines for the automatic use of these formulas in the Web documents in the medical domain.
We take advantage of the framework built to further investigate other understandability estimators besides the readability formulas.
Finally, we use the lessons learnt in our correlation analysis to demonstrate how is possible to retrieve more understandable medical content. 

\todo{add roadmap?}

%We start our experiments in this chapter analysing the correlation of each readability formula to the human assessments, our ground truth, in Section~\ref{sec:which_readability}.
%We then study other understandability estimators going far beyond the readability formulas. Those are presented in Section~\ref{sec:proxies}.
%Similarly, to our experiments with readability formulas, we empirically investigate in Section~\ref{sec:beyond_readability} the patterns that lead to effective understandability estimators.
%In Section~\ref{sec:which_preprocessing} we compare the effect of different preprocessing methods in the light of more than 100 understandability estimators.
%Our findings and highlights are summarized in Section~\ref{sec:conclusion_doc_analysis}.

\section{Related Word}
\label{sec:related}
Readability formulas? CLEF work? CIKM paper? White/MSR papers?....???

\section{Which Readability Formula To Use}
\label{sec:which_readability}

%The aim of this section is to study the individual correlation of each traditional readability metric listed in Table~\ref{tab:doc_features}.
%Our goal is to provide guidelines for the use of readability metrics on health Web documents, as typically has been done in various recent works 
%(e.g., \cite{graber99readability,fitzsimmons2010readability,wiener2013readability,patel13readability,atcherson14readability,meillier17readability}) \todo{elaborate on this motivation}. 
%We plotted Figures~\ref{fig:bar_corr_clef15} and~\ref{fig:bar_corr_clef16} focusing on the correlation score of each traditional readability metric of Figures~\ref{fig:boxplot_corr_docs_2015} and~\ref{fig:boxplot_corr_docs_2016}.
% TODO: write about the preprocessing methods here...

Palotti, Zuccon and Hanbury~\cite{palotti15} compared six different HTML cleaning methods and their impact in the use of readability formulas.
They evaluate three methods to extract the content of a Web page from its HTML source: BeautifulSoap 4, which just naively removed HTML tags, Boilerpipe\footnote{Add ref.} and Justext\footnote{Add ref.}, two approaches to eliminate boilerplate text together with HTML tags. Henceforth these methods are referred as \textit{Naive}, \textit{Boilerpipe - Boi} and \textit{Justext - Jst}, respectively.
The authors noticed that the text in HTML tags often missed a correct punctuation mark. For example, the text extract from titles, menus, tables or lists could be interpreted as many short sentences of few very long sentences, depending only whether a period is forced at the end of sentences. These two preprocessing options are henceforth called \textit{ForcePeriod - FP} and \textit{DoNotForcePeriod - DNFP}.

Their experiments found that the use of \textit{Naive} preprocessing was associated with larger variances in the understandability score estimated by readability formulas, while the use of both \textit{Boi} and \textit{Jst} were more stable. Coleman-Liau Index (CLI) was the most stable metric among all tested in Palotti's work\cite{palotti15}.

In our experiments, we use the Pearson, Kendall Tau and Spearman's Rank correlation to compare the understandability label assigned to each document by human assessors and by readability formulas.
Pearson correlation is used to calculate the strength of two linearly related variables, while the Kendall and Spearman are rank correlation, i.e., they act on the rank of the variables instead of their values. We opt to report on all three correlation coefficients as all three are equally used in the literature, and thus we allow other researchers to compare our results across.

We show in Figures~\ref{fig:bar_corr_clef15} and~\ref{fig:bar_corr_clef16} the correlation scores of each traditional readability metric with the human assessments made in CLEF 2015 and 2016, respectively. We observe that the \textit{Naive} preprocessing also results in the lowest
correlation, no matter which correlation measure or readability formula is used. Also, when the \textit{Naive} preprocessing is used, the variant \textit{DoNotForcePeriod} yields higher correlations than the variant \textit{ForcePeriod}, but when using a higher quality HTML cleaner, such as Justext or Boilerplate, the results indicate that the use of \textit{ForcePeriod} should be preferred.

Among all the readability formulas and preprocessing methods, SMOG with \textit{ForcePeriod} preprocessing and Dale-Chall Index with \textit{DoNotForcePeriod} are the best ones respectively for 2015 and 2016. Although there is no single best readability measure or best preprocessing strategy in all scenarios, CLI and FRE with \textit{Justext} are stable options, with correlation coefficients as high as the best ones in both campaigns. Thus, we confirm Palotti's advice for the use of CLI, as it
has shown once more to be the most robust measure to variances due to use of \textit{ForcePeriod} or \textit{DoNotForcePerid}.


\begin{figure}[th!]
   \centering
   \includegraphics[width=1.\textwidth]{graphics/bar_corr_pearson15_values}
   \includegraphics[width=1.00\textwidth]{graphics/bar_corr_spearman15_values}
   \includegraphics[width=1.00\textwidth]{graphics/bar_corr_kendalltau15_values}
   \includegraphics[width=1\textwidth]{graphics/legend62}
    \caption{Correlation of different readability measures and the understandability scores collected in CLEF eHealth 2015}
   \label{fig:bar_corr_clef15}
\end{figure}

\begin{figure}[th!]
   \centering
    \includegraphics[width=1.\textwidth]{graphics/bar_corr_pearson16_values}
    \includegraphics[width=1.\textwidth]{graphics/bar_corr_spearman16_values}
    \includegraphics[width=1.\textwidth]{graphics/bar_corr_kendalltau16_values}
    \includegraphics[width=1\textwidth]{graphics/legend62}
    \caption{Correlation of different readability measures and the understandability scores collected in CLEF eHealth 2016.}
   \label{fig:bar_corr_clef16}
\end{figure}


\section{(More) Understandability Estimators}
\label{sec:proxies}

The correlation of readability formulas as shown in Figures~\ref{fig:bar_corr_clef15} and~\ref{fig:bar_corr_clef16} is not strong, with no correlation coefficient being higher than 0.5.
Our next intent is comparing the correlation coefficient of the traditional readability formulas with other methods for understandability estimation, including an evaluation of other humans performing the same task.
For that, we devise and group several methods into semantically related groups which will be following presented.
We summarize all methods in Table~\ref{tab:doc_features}.

%Therefore, what unifies all methods listed in this section is the goal to automatically infer the understandability of a document, in our case, a Web page with medical content.
%For sake of understanding, in Table~\ref{tab:doc_features} we list all the methods used in this chapter to estimate document understandability.
%Note that we divide these estimators into semantically related groups, which are presented below.

\subsection{Traditional Readability Formulas}
This group contains all the readability formulas listed in Chapter~\ref{chp:preprocessing} (Table~\ref{tab:formulas}) and additional readability formulas that were excluded for not being on the same value scale as the other ones.

\subsection{Raw Components of Readability Formulas}
This group comprises the building blocks that make up the traditional readability measures. Some examples include the average number of characters per word or the average number of syllables in a sentence\footnote{Words were divided into syllables using the Python package Pyphen \url{http://pyphen.org/}}.

\subsection{General Medical Vocabularies}
This group includes methods such as the number of words with a medical prefix or suffix, i.e. beginning or ending with Latin or Greek particles (e.g., amni, angi, algia, arteri), acronyms \footnote{The acronym list was obtained from the ADAM database \cite{zhou2006}} or medical vocabularies such as the International Statistical Classification of Diseases and Related Health Problems (ICD), Drugbank and the OpenMedSpel dictionary\footnote{\url{http://extensions.openoffice.org/en/project/openmedspel-en-us}}. 
Methods listed here were matched with documents using a simple keywords matching.

\subsection{Consumer Vocabulary Features}
the Consumer Health Vocabulary (CHV) is a prominent medical vocabulary dedicated to mapping consumer (layperson) vocabulary to technical terms. 
It attributes a score for each of its concepts with respect to their difficulty, with lower/higher scores for harder/easier concepts.
We used MetaMap once again to map the content of Web documents, as done in Chapters~\ref{chp:user_query_logs} and~\ref{chp:medical_expertise}. 
We further use MetaMap options to also filter only concepts identified as symptoms or diseases, using the same definitions from Section~\ref{sec:semantic}. 
%The CHV dataset (version 20110204) links part of the UMLS concepts, such as “myocardial infarction”, to everyday expressions, “heart attack”.

\subsection{Expert Vocabulary Features}

The hierarchy of Medical Subject Headers (MeSH) was previously used in the literature to identify hard concepts, assuming that a concept that is deep in the hierarchy is harder than a shallow one~\cite{yan11}.
As done with CHV, we used MetaMap to map the content of Web documents to MeSH and explore symptoms and disease concepts separately.

\subsection{Natural Language}
This group comprises commonly used metrics in the natural language processing field: the ratio of part-of-speech (POS) classes, the number of entities in a text, the sentiment polarity and the ratio of words found in English vocabularies. The Python package NLTK 3.2\footnote{\url{http://www.nltk.org/}} was employed for sentiment analysis and POS tagging. The GNU Aspell\footnote{\url{http://www.aspell.net/}} dictionary was used as a standard English vocabulary and a stopword list was built by merging the stopword
lists of the Indri\footnote{\url{http://www.lemurproject.org/indri/}} and Terrier\footnote{\url{http://www.terrier.org/}} toolkits. 

\subsection{HTML Features}
The aim of this group is to represent a web page by its HTML content.
We hypothesize that a Web page rich of images or with its content well summarized in tables can potentially ease hard subjects such as medicine. 
We identify a large number of HTML tags in this group with the Python library BeautifulSoap v4.4\footnote{\url{https://www.crummy.com/software/BeautifulSoup/}}.

\subsection{Word Frequency Features}
\label{sec:word_freq}

Common and known words are usually frequent words, while unknown and obscure words are rare. This idea is implemented in readability formulas such as the Dale-Chall index which uses a list of common words and counts the number of words that fall outside this list~\cite{dale48}.
In this work we model word frequency in a straightforward manner: we sort the frequency of all words in a corpus and normalize the ranking of word frequency such that values close to 100 are attributed to common words and values close to 0 to rare words. 
We explore three different corpora in this work:

\begin{itemize}
\item Medical Reddit: Reddit is an Internet forum with a sizable user community which is responsible for generating content. Any user can start a discussion receiving replies from any other user.
This discussion forum is intensively used for health purposes, for example in the Reddit community AskDocs licensed nursers and doctors (subject to user identity verification) advise help seekers free of charge. We selected six of such communities (medical, AskDocs, AskDoctorSmeeee, Health, WomensHealth, Mens\_Health) and downloaded all user interactions using the Python Reddit API Wrapper (PRAW\footnote{\url{https://praw.readthedocs.io/}}), v5.1. In total 43,018 discussions were collected.

\item PubMed Central: PubMed Central (PMC) is an online digital database of freely available full-text biomedical literature playing a similar role to physicians as the ACM Digital Library does to computer scientists. We used in this work the same collection crafted for the TREC Clinical Decision Support Track 2014 and 2015 (TREC-CDS)\footnote{\url{http://www.trec-cds.org/}} consisting of 733,138 articles. 
 
\item Medical English Wikipedia: we filtered articles from a Wikipedia dump\footnote{\url{https://dumps.wikimedia.org/enwiki/}} (May 1st 2017), that contained an Infobox\footnote{A Wikipedia infobox is a template containing structured information that appear on the right of Wikipedia pages to summarize key aspects of concepts} in which at least one of the following words appeared as a property: ICD10, ICD9, DiseasesDB, MeSH, MeSHID, MeshName, MeshNumber, GeneReviewsName, Orphanet, eMedicine, MedlinePlus, drug\_name, Drugs.com, DailyMedID, LOINC.
Figure~\ref{fig:hyperthermia} illustrates a Wikipedia page that is marked as medical because of its Infobox entries.
This idea was successfully implemented in Soldaini et al.~\cite{soldaini15} and our filtering process resulted in a collection of 11,942 articles. 
Note that this procedure highly favors precision over recall.
   
\end{itemize}

A summary of the statistics of these three collections is reported in Table~\ref{tab:collection_stats}.
In order to calculate word frequency, we removed words that occur less than 5 times in a corpus.
Finally, we ignore out of vocabulary (OV) words in our calculations, unless it is explicitly stated.

\begin{figure}[th!]
   \centering
   \includegraphics[width=0.50\textwidth]{graphics/hyperthermia}
    \caption{Wikipedia page on hyperthermia. A rectangular red box identify the Infobox on the right hand side containing entries for Specialty, ICD-9-CM, DiseasesDB and MeSH.}
    \label{fig:hyperthermia}
\end{figure}

\input{tab_collection_stats}

\subsection{Machine Learning on Text - Regressors and Classifiers}

In a recent survey, Kevin Collins-Thompson reports that the future of understandability estimation relies on Machine Learning~\cite{collins2014computational}.
A challenge in using Machine Learning in this task is defining the background corpora used as training set.
A possible setup for our work could have used CLEF 2015 assessments to learn a model for CLEF 2016 and vice-versa, but instead, we opt for a 
more reusable solution for the medical/health domain. 
We employed the three datasets explained in Section~\ref{sec:word_freq} and assume different labels according to the average difficulty of documents in these collections:

\begin{itemize}
    \item Medical Reddit (label 1): Documents in this collection are expected to be written in a colloquial style, and thus the easiest to understand. All the conversations are in fact explicitly directed to assist inexpert health consumers;
    \item Medical English Wikipedia (label 2): Documents in this collection are expected to be less formal than scientific articles, but more formal than a Web forum like Reddit;
    \item PubMed Central (label 3): Documents from this collection are expected to be written in a highly formal style, as the target audience of these documents are physicians, nursers and researchers in the biomedical domain.
\end{itemize}

Models were trained on a Latent Semantic Analysis (LSA) empirically set to have ten dimension based on word counts in documents in these three collections.
We model two different tasks a classification one and a regression task.
Different labels for the regression could be employed, for example, a label 5 to PubMed Central documents to emphasize that these documents are explicitly made for expert users.
We did not explore the effects of different labels in this work, it is left as future work.

\input{tab_doc_features}

\section{Top Measures from Each Group}
\label{sec:beyond_readability}

We correlated each individual understandability estimator listed in Table~\ref{tab:doc_features} with the human assessments collected in CLEF eHealth 2015 and 2016 campaigns.
We report in Table~\ref{tab:top_corr_metrics} the best metric for each group according to Pearson, Spearman or Kendall correlation.
For some groups, such as the readability formula group, the highest correlated metric was the same for different correlation measure: SMOG Index in CLEF eHealth 2015 and Dale-Chall Index in 2016. 
We highlight the top score value of each correlation measure in each group. Note that there is no single case in which three different metrics were the top correlated for each different correlation measure.
\todo{hypotesis that kendatll tau and spearman always point to the same winner}

Interestingly, Table~\ref{tab:top_corr_metrics} shows that the polysyllable words, best formula component metric for CLEF 2015 data, is the main metric for the SMOG formula, the best readability formula for CLEF 2015. 
Likewise, the number of difficult words, best formula component metric for CLEF 2016, is the main metric for Dale-Chall index, the best readability formula for CLEF 2016.

The top correlation for MeSH group, number of MeSH concepts, reaches much lower correlation than the top correlation metric for the CHV group, the scores of CHV concepts.
The dominating metrics for the Natural Language group are the number of pronouns, the number of stopwords and the number of out of vocabulary words; all these are consistently more correlated than metrics in the MeSH and CHV group.
In turn, the top correlations for the HTML group, counts of P tags and list tags, were the weakest. P tags are used to create paragraphs in a Web page, being roughly a proxy for text lengthiness. 
Top estimators for the word frequency group are based on the Medical Reddit and PubMed counts, with correlations as high as the readability formulas.
Finally, the group with the highest correlated estimators are the regressors and classifiers, with top estimators being the Neural Network regressor and the multinomial Naive Bayes.
\todo{this section misses some sort of conclusion or at least a link to the next section}
%
\input{tab_top_corr_metrics}
%

\section{Which Preprocessing Approach To Prefer}
\label{sec:which_preprocessing}

We further investigate the preprocessing steps with the groups of features introduced in Table~\ref{tab:doc_features}.
For that, we present in Figures~\ref{fig:boxplot_corr_docs_2015} and~\ref{fig:boxplot_corr_docs_2016} the box plot of different correlation metrics divided by preprocessing alternative for CLEF eHealth 2015 and 2016. 
For instance, the very first box plot in the upper part of these figures shows the absolute Pearson's rank correlation of different readability metrics when using a combination of Naive and ForcePeriod as preprocessing steps.
Boxes extend from the lower to upper quartile values of the data, with a line at the median. Whiskers extend from the box to show the range of the data. Flier points are those past the end of the whiskers, usually interpreted as outlier values.

We also include in Figures~\ref{fig:boxplot_corr_docs_2015} and~\ref{fig:boxplot_corr_docs_2016} boxes for the summary of the 3 preprocessing procedures to remove HTML, the use of HTML features, which is done without any preprocessing and the comparison with other human assessors. For CLEF eHealth 2015, we used as human assessments the additional assessments made by unpaid medical students and health consumers (see~\cite{palotti16b}), while for CLEF eHealth 2016 data, we randomly selected 100 pages that were assessed by another assessor. \mytodo{add at least another person doing assessments}.
The correlations with human assessments provide important insights on how hard and subjective understandability assessments are.

\begin{figure}[th!]
   \centering
   \includegraphics[width=0.70\textwidth]{graphics/box_pearson15_raw_values}
   \includegraphics[width=0.70\textwidth]{graphics/box_spearman15_raw_values}
   \includegraphics[width=0.70\textwidth]{graphics/box_kendalltau15_raw_values}
    \includegraphics[width=0.65\textwidth]{graphics/legendCorr}
    \vspace{-1.cm}
    \caption{Box plots divided by feature groups. Correlations are calculated using understandability labels from relevant documents assessed in CLEF eHealth 2015}
   \label{fig:boxplot_corr_docs_2015}
\end{figure}

\begin{figure}[th!]
   \centering
   \includegraphics[width=0.70\textwidth]{graphics/box_pearson16_raw_values}
   \includegraphics[width=0.70\textwidth]{graphics/box_spearman16_raw_values}
   \includegraphics[width=0.70\textwidth]{graphics/box_kendalltau16_raw_values}
    \includegraphics[width=0.65\textwidth]{graphics/legendCorr}
    \vspace{-1.cm}
    \caption{Box plots divided by feature groups. Correlations are calculated using understandability labels from relevant documents assessed in CLEF eHealth 2016}
   \label{fig:boxplot_corr_docs_2016}
\end{figure}

Figure~\ref{fig:boxplot_corr_docs_2015} shows the correlations for CLEF eHealth 2015 assessments.
The choice of preprocessing method had the highest impact on the traditional readability formula group, with the Naive preprocessing clearly underperforming the other preprocessing methods. The choice of the Naive method was also the worst with the raw readability formula components and word frequency estimators, but, interestingly, it was a good choice, if not the best one, for all other groups.
The highest correlations were archived by the regressors and classifiers, independently of the preprocessing method used.

%For this group, all correlation measures point out that the Naive processing yielded the weakest correlation, and Justext was marginally better than Boilerpipe. Comparing the medians, the strategy of DoNotForcePeriod performed better than ForcePeriod. The readability formula group was also the one with higher correlation, with an average correlation equal or higher than the human one.

Similarly to Figure~\ref{fig:boxplot_corr_docs_2015}, Figure~\ref{fig:boxplot_corr_docs_2016} reports the findings for CLEF eHealth 2016. This time, though, the Naive preprocessing method was clearly underperforming for most of the groups analysed, including regressors and classifiers.

In order to further understand our experiments, we compared the median of each pair of preprocessing strategy showed in Figures~\ref{fig:boxplot_corr_docs_2015} and~\ref{fig:boxplot_corr_docs_2016} and present the results in Table~\ref{tab:comparison_preprocessing}. 
For instance, the entry \textit{FP < DNFP} counts the number of times the median value for ForcePeriod was superior to DoNotForcePeriod when comparisons with the same HTML cleaning method was used, e.g. Naive ForcePeriod versus Naive DoNotForcePeriod. From all comparisons, the ones that were statistically significant according to a t-test are shown inside parentheses.

The upper part of Table~\ref{tab:comparison_preprocessing} shows results for the comparisons between ForcePeriod (FP) and DoNotForcePeriod (DNFP). Although the interpretation of readability formulas is drastically affected by this choice of preprocessing, as learning in Chapter~\ref{chp:preprocessing}, the correlation results are not.
The number of times FP reached a higher correlation than DNFP is roughly the same that DNFP was higher than FP.
The bottom part of Table~\ref{tab:comparison_preprocessing} shows the comparisons made for Naive, Justext and Boilerpipe. Results for CLEF 2015 contrast with 2016, while Naive was sightly better than Boilerpipe and Justext in 2015, it was the worst in almost all 2016 comparisons. Also, the comparisons between Justext and Boilerpipe are exactly the opposite from 2015 to 2016.

%
\input{tab_comparison_preprocessing}
%

\section{Experimenting with Understandability}
\label{sec:experiments}

The data use made our analysis here was also used in the Information Retrieval branch of CLEF eHealth.
We focus our attention to the CLEF eHealth 2016 campaign leaving 2015 experiments offline\footnote{Link to experiments will be available upon acceptance of this manuscript}.

We start by the defining the evaluation measure that we will use here. 
In CLEF eHealth campaign, organizers used a modification of RBP which ties together the relevance of a document with any other relevance dimension, in this case in particular, with understandability.
Mathematically, it consists in adding an understandability factor to the RBP formula, as shown:
\todo{add formula here}

The drawback of such evaluation metric is that we cannot separately evaluate each dimension. We propose, instead, to separately evaluate a ranking list with respect to its topical relevance and its understandability:
\begin{itemize}
        \item P@10r: a document is topically relevant if assessed as somewhat relevant or highly relevant. This metric counts the number of relevant documents in the top 10 documents of a ranked list.
        \item P@10u: a document is relevant for this metric if the understandability score is smaller than a threshold $U$. Like P@10r, we count the number of relevant documents in the first 10 docs of a ranked list. We use $U = 40$ in our experiments. \todo{I decided to use this threshold based on the data. I will need to add a figure to support this clam, I think.}
\end{itemize}

During the campaign, organizers opt to use shallow pools and focus on highly ranked documents, using P@10r as one official metric for topical relevance.
It makes our choice of metric natural. Likewise it is traditionally done with F measure, we combine P@10r and P@10u with an harmonic mean: $F_ru = 2 \times \frac{P@10r \times P@10u}{P@10r + P@10u}$


\input{tab_experiments}



\section{Conclusion}
\label{sec:conclusion_doc_analysis}

There is an abundance of factors that affect how readability is perceived by users. 
In this chapter we devised and studied a large number of readability estimators, ranging from traditional readability formulas extensively used in the past 50 years to state-of-the-art machine learning algorithms.
We grouped them into semantically related groups in order to visualize their correlation with human assessments collected during CLEF eHealth campaigns in 2015 and 2016.

Complementary to our previous chapter, we evaluated how preprocessing steps impact the readability estimation in traditional readability formulas and in other modern estimators. We empirically learnt the importance of preprocessing steps when applying readability formulas, as the highest correlations happen when other than the Naive method is used.
For the most modern estimators, such as the ones based on machine learning methods, the correlation is less sensible to the preprocessing steps.

We also studied the correlation of each individual readability formula to the human assessment to provide insights on which formula should be preferred. Our analysis concluded that the Simple Measure of Gobbledygook (SMOG) and Dale-Chall Index (DCI) were the most correlated metrics for the two datasets studied and, together with Coleman-Liau Index (CLI) and the Flesch Reading Ease (FRE) are the most stable metrics across datasets, and therefore, should be preferred.

Finally, this chapter serves as a basis for the following chapters of this work, as the learning to rank methods will largely use the estimators devised and analysed here.

%MISSING - POSSIBLE TODOs:
% * I will use the evaluations from CLEF eHealth 2015 and 2016 to conclude that for the same topic, we can find very different results in respect to their understandability (readability) score. I might want to hire some people to classify how hard a topic is. Then I could correlate that with the mean scores of documents for that topic.
% * I will try to come up with some sort of reasoning behind the understandability judgements: is it because of the text length? number of hard (long or out of dictionary) words?

%\mytodo{In this chapter I could add other readability measures later, such as the neural network ones (see cikm 2017)}





\bibliographystyle{abbrv}
\bibliography{main}

\end{document}
