
\section{ Understandability Estimators}
\label{sec:proxies}
%The previous results highlighted that surface level readability formulas only weakly correlate with human assessments of Web pages understandability (Figure~\ref{fig:bar_corr_clef15}). Next, we considered alternative methods of estimating understandability (Table~\ref{tab:doc_features}), which we present in the following, grouped into categories. 

As reviewed in Section~\ref{}, several methods have been proposed and used to estimate the understandability of health Web pages, with the most popular methods (at least in the biomedical literature) being readability formulas based on surface level characteristics of text. Next, we outline the categories of methods to estimate understandability used in this work; an overview is shown in Table~\ref{tab:doc_features}). Some of the methods presented here further expand measured used in the literature. 


 
%The correlation of readability formulas as shown in Figure~\ref{fig:bar_corr_clef15} %and~\ref{fig:bar_corr_clef16} is not strong, without any correlation coefficient being higher than 0.5.
%Our next intent is comparing the correlation coefficient of the traditional readability formulas with other methods for understandability estimation, including an evaluation of other humans performing the same task.
%For that, we devise and group several methods into semantically related groups which will be following presented. We summarize all methods in Table~\ref{tab:doc_features}.


\textbf{Traditional Readability Formulas (RF):}
These included the readability formulas used in Section~\ref{sec:related}, as well as other, less popular ones. A full list is provided in surveys by Collins-Thompson~\cite{collins2014computational} and Dubay~\cite{dubay04}.

%Therefore, what unifies all methods listed in this section is the goal to automatically infer the understandability of a document, in our case, a Web page with medical content.
%For sake of understanding, in Table~\ref{tab:doc_features} we list all the methods used in this chapter to estimate document understandability.
%Note that we divide these estimators into semantically related groups, which are presented below.

%\textbf{Traditional Readability Formulas:}
%This group contains a large set of traditional readability formulas. Some were mentioned in Section~\ref{sec:related}. A full list can be found in surveys such as Collins-Thompson~\cite{collins2014computational} or Dubay~\cite{dubay04}.

\textbf{Raw Components of Readability Formulas (CRF):}
These were formed by the ``building blocks'' used in the traditional readability formulas; examples of such building blocks included the average number of characters per word and the average number of syllables in a sentence. Words were divided into syllables using the Python package Pyphen~\cite{pyphen}.


%\textbf{General Medical Vocabularies:}
%This group includes methods such as the number of words with a medical prefix or suffix, i.e. beginning or ending with Latin or Greek particles (e.g., amni-, angi-, algia-, arteri-), acronyms or medical vocabularies such as the International Statistical Classification of Diseases and Related Health Problems (ICD), Drugbank and the OpenMedSpel dictionary \cite{openmedspel}.
%Acronym list was obtained from ADAM database~\cite{zhou2006}. Methods listed here were matched with documents using a simple keywords matching.


\textbf{General Medical Vocabularies (GMV):}
These included methods that count the number of words with a medical prefix or suffix, i.e. beginning or ending with Latin or Greek particles (e.g., amni-, angi-, algia-, arteri-), text strings included in list of acronyms or medical vocabularies such as the International Statistical Classification of Diseases and Related Health Problems (ICD), Drugbank and the OpenMedSpel dictionary~\cite{openmedspel}. An acronym list was obtained from the ADAM database~\cite{zhou2006}. Methods in this group were matched with documents using a simple keywords matching.




%\textbf{Consumer Vocabulary Features:}
%NLM MetaMap~\cite{aronson10} was employed to map the content of Web documents to the CHV vocabulary~\cite{zeng06}.
%We further use MetaMap options to also filter only concepts identified as symptoms or diseases.
%Similar approach is commonly used in the literature \cite{} \mytodo{maybe cite something that is not ours}.
%The CHV dataset (version 20110204) links part of the UMLS concepts, such as “myocardial infarction”, to everyday expressions, “heart attack”.


textbf{Consumer Medical Vocabulary (CMV):}
The popular MetaMap~\cite{aronson10} tool was used to map the text content of Web pages to entries in the CHV vocabulary~\cite{zeng06}.
We further used the MetaMap options to retain only concepts identified as symptoms or diseases. Similar approaches have been commonly used in the literature\todo{~\cite{}}.



\textbf{Expert Medical Vocabulary (EMV):}
Similarly to the CHV features, we used MetaMap to convert the content of Web pages into MeSH entities, studying symptoms and disease concepts separately. 



\textbf{Natural Language (NL):}
These included commonly used natural language heuristics such as the ratio of part-of-speech (POS) classes, the number of entities in the text, the sentiment polarity and the ratio of words found in English vocabularies. The Python package NLTK~\cite{nltk} was employed for sentiment analysis and POS tagging. The GNU Aspell~\cite{aspell} dictionary was used as a standard English vocabulary and a stop word list was built by merging those of Indri~\cite{indri} and Terrier~\cite{terrier}. 



\textbf{HTML Features (HF):}
These included the identification of a large number of HTML tags, which were extracted with the Python library BeautifulSoap~\cite{bs4}. The intuition for these features is that Web pages rich of images of tabular content may well explain and summarise health content -- thus providing more understandable content to the general public. 
%The aim of this group is to represent a web page by its HTML content.
%We hypothesize that a Web page rich of images or with its content well summarized in tables can potentially ease hard subjects such as medicine. 
%We identify a large number of HTML tags in this group with the Python library BeautifulSoap \cite{bs4}.


\textbf{Word Frequency Features (WFF):}
Generally speaking, common and known words are usually frequent words, while unknown and obscure words are generally rare. This idea is implemented in readability formulas such as the Dale-Chall index which uses a list of common words and counts the number of words that fall outside this list (complex words)~\cite{dale48}.
We extended these observations by studying corpora-wide word frequencies. 
We modelled word frequencies in a corpus in a straightforward manner: we sorted the word frequencies and normalized word rankings such that values close to 100 were attributed to common words and values close to 0 to rare words. Three corpora were analysed to extract word frequencies:


\begin{itemize}
\item \underline{Medical Reddit:} Reddit~\cite{reddit} is a Web forum with a sizeable user community which is responsible for generating and moderating its content. Any user can start a discussion or reply to a discussion. This forum is intensively used for health purposes, for example in the Reddit community AskDocs~\cite{redditaskdocs} licensed nursers and doctors (subject to user identity verification) advise help seekers free of charge. We selected six of such communities
    (medical, AskDocs, AskDoctorSmeeee, Health, WomensHealth, Mens\_Health) and downloaded all user interactions available until \todo{ DATE} using the Python library PRAW~\cite{redditapi}. In total 43,018 discussions were collected.



\item \underline{PubMed Central:} PubMed Central~\cite{pubmed} is an online  database of freely available full-text biomedical literature. We used the same collection distributed for the TREC 2014 and 2015 Clinical Decision Support Track (TREC-CDS)~\cite{roberts16,trec15}, consisting of 733,138 articles. 
 
\item \underline{Medical English Wikipedia:} after obtaining a recent  Wikipedia dump~\cite{wikipedia} (May 1st 2017), we filtered articles to only those  containing an Infobox\footnote{A Wikipedia infobox is a template containing structured information that appears on the right of Wikipedia pages to summarize key aspects of articles.} in which at least one of the following words appeared as a property: ICD10, ICD9, DiseasesDB, MeSH, MeSHID, MeshName, MeshNumber, GeneReviewsName, Orphanet, eMedicine, MedlinePlus, drug\_name, Drugs.com, DailyMedID, LOINC.
%Figure~\ref{fig:hyperthermia} illustrates a Wikipedia page that is marked as medical because of its Infobox entries.
In doing so, we followed the method by Soldaini et al.~\cite{soldaini15}, which favours precision over recall when identifying an health-related article. This resulted in a collection of 11,942 articles. 
%Note that this procedure highly favors precision over recall. %\mytodo{In case we need space, I suggest we drop this figure}
\end{itemize}

A summary of the statistics of these three collections is reported in Table~\ref{tab:collection_stats}. To calculate word frequencied, we removed words that occurred less than 5 times in a corpus. Finally, unless explicitly stated otherwise, we ignored out of vocabulary (OV) words in our feature calculations.



%\begin{figure}[th!]
%   \centering
%   \includegraphics[width=0.50\textwidth]{graphics/hyperthermia}
%    \caption{Wikipedia page on hyperthermia. A rectangular red box identify the Infobox on the right hand side containing entries for Specialty, ICD-9-CM, DiseasesDB and MeSH.}
%    \label{fig:hyperthermia}
%\end{figure}

\input{tables/tab_collection_stats}

\textbf{Machine Learning on Text - Regressors (MLR) and Classifiers (MLC):} These included machine learning methods for estimating Web page understandability. While Collins-Thompson highlighted the promise of estimating understandability using machine learning methods, a challenge is identifying the background corpus to be used for training~\cite{collins2014computational}. To this aim, we used the three corpora detailed above, and assume understandability labels according to the average (expected) difficulty of documents in these collections:


%In a recent survey, Kevin Collins-Thompson reports that the future of understandability estimation relies on Machine Learning~\cite{collins2014computational}.
%A challenge in using Machine Learning in this task is defining the background corpora used as training set.
%A possible setup for our work could have used CLEF 2015 assessments to learn a model for CLEF 2016 and vice-versa, but instead, we opt for a 
%more reusable solution for the medical/health domain. 
%We employed the three datasets described in Table~\ref{tab:collection_stats} and assume different labels according to the average difficulty of documents in these collections:



\begin{itemize}
    \item Medical Reddit (label 1): Documents in this collection were expected to be written in a colloquial style, and thus the easiest to understand. All the conversations were in fact explicitly directed to assist inexpert health consumers;
    \item Medical English Wikipedia (label 2): Documents in this collection were expected to be less formal than scientific articles, but more formal than a Web forum like Reddit, thus somewhat more difficult ro understand;
    \item PubMed Central (label 3): Documents from this collection were expected to be written in a highly formal style, as the target audience were physicians and researchers in the biomedical domain.
\end{itemize}

Models were trained using Latent Semantic Analysis (LSA) with 10 dimensions (empirically set based on document word counts in the three collections).
We modelled two different tasks: a classification and a regression task. \todo{this sentence about tasks is coming out the blue: what tasks? What do they do? For what?. Also, it is unclear what these regressors and classifiers are.}


%Different labels for the regression could be employed, for example, a label 6 to PubMed Central documents would emphasize that these documents are explicitly made for expert users, being 3 times harder than Wikipedia ones. We did not explore the effects of different labels in this work, it is left as future work.

\input{tables/tab_doc_features}


