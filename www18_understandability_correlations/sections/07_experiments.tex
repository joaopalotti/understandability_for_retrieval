\section{Integrating Understandability into Retrieval}
\label{sec:experiments}

Next, we investigate how understandability estimations could be used and integrated into retrieval methods to increase the quality of search result. 
%
%We investigate here two real-world scenarios depending on the control a developer/researcher has over their system: 
%\begin{itemize}
%    \item Partial control: the developer/researcher is running a retrieval system which retrieves a ranked list of documents (not necessarily the score of each document) for a user query. 
%    \item Total control: the developer/researcher is an Information Retrieval expert and can implement/use a learning to rank system.
%\end{itemize}
%
%
%To achieve this, we applied selected understandability estimation approaches to improve the results of the selected retrieval runs. 

We introduce, as our first approach, a simple and effective method to explicitly increase understandability based on any understandability estimator. 
Given a pre-defined cut-off parameter, our approach blindly sorts the top retrieved documents up to this cut-off based on the understandability estimator.
With the job of retrieving topically relevant documents is left to the underline search engine, our approach focus on retrieving understandable documents.

An advantage of this approach is that it can be applied to any ranked list.
We evaluated the best readability formula for 2016 collection (Dale Chall, Table~\ref{tab:top_corr_metrics}) and several machine learning methods trained on top of a subset (or all) estimators listed in Table~\ref{tab:doc_features}. We model the machine learning experiments here as a regression task, which uses as training data the labels of documents assessed in CLEF 2016.
We predict scores for documents in the training set with a 10-fold cross validation and predict scores for documents outside the training set using all available data.

Our hypothesis is that such re-rank based solely on a understandability score is able to increase the understandability of the results ($P_u@10$) at the expenses of topical relevance ($P_r@10$). The understandability-relevance trade-off is primarily controlled by the cut-off. A relatively large cut-off, fifty, for instance, would allow the re-rank method to move a document from the 5th resulting page (considering the default 10 results per page) to the first one depending on the understandability score assigned to that document. We experiment with cut-offs at 15, 20 and 50 documents. 

Estimating a good cut-off parameter might be costly for a real world application...Alternatively to the cut-off control, we propose to automatically fuse the results of the relevance-optimized-system and the understandability-optimized-system into a single fused one.
This merge aims to retain the best of both extremes into a single system. For this, we used the Reciprocal Rank Fusion (RRF) method~\cite{cormack09}, which effectively combines two lists of search results based on the position of the results in a ranking.

Our second approach investigates how to blend understandability and relevance in a single step with learning to rank.

\todo{--- Stopped Here ---}

In the LTR we need to mention the combination of labels to make up the final document label which will be used by the method. Also, we need to say which method we used (pairwise tree boosting) and which are the features for each run....


In our experiments, we selected the top two systems at CLEF 2016 (based on their performance on $P@10$ \todo{check if they are the best systems for uRBP as well. It is likely.}) 
and created a plain BM25 system using the Terrier toolkit~\cite{terrier}. While we do not have access to the implementation of the first two systems, we have completely control over this BM25 system which will be useful later in our experiments.



%We explored both estimations based on readability formulas (we used Dale-Chall) and on machine learning regressors (we used linear regression). 
%These methods were used for re-ranking the top 20 search results returned by the initial systems. 

In the following we report results for CLEF 2016. Results for CLEF 2015 are included in the online appendix: they show similar trends to the 2016 results.

%Our experiments are based on the data collected during the Information Retrieval branch of CLEF eHealth. To the best of our knowledge that was the only venue in which topical and understandability assessments were collected. 
%We show here experiments with the CLEF eHealth 2016 campaign leaving 2015 experiments offline\footnote{Link to experiments will be available upon acceptance of this manuscript}.
%
%We start by the defining the evaluation measure that we will use here. 
%In CLEF eHealth campaign, organizers used a modification of RBP which ties together document relevance any other relevance dimension, in this case in particular, understandability \cite{clef16}.
%Mathematically, it consists in adding an understandability factor to the RBP formula: $uRBP(\rho) = (1 - \rho) \sum_{k=1}^{K} \rho^{K-1} r(d@K) u(d@k)$, with r(d@k) representing the gain in retrieving a topically relevant documents at rank $k$ and u(d@k) the gains coming from the understandability factor of a document at rank $k$.
%
%The drawback of such evaluation metric is that we cannot separately evaluate each dimension. We propose, instead, to separately evaluate a ranking list with respect to its topical relevance and its understandability:
%\begin{itemize}
%        \item $P_r@10$: a document is topically relevant if assessed as somewhat relevant or highly relevant. This metric counts the number of topically relevant documents in the top 10 documents of a ranked list.
%        \item $P_u@10$: a document is relevant for this metric if the understandability score is smaller than a threshold $U$. Like $P_r@10$, we count the number of relevant documents in the first 10 documents of a ranked list. We use $U = 40$ in our experiments. \todo{I decided to use this threshold based on the data. I will need to add a figure to support this clam, I think.}
%\end{itemize}
%
%During the campaign, organizers opt to use shallow pools and focus on highly ranked documents, using $P_r@10$ as one official metric for topical relevance.
%It makes our choice of metric natural. Likewise it is traditionally done with F measure, we combine $P_r@10$ and $P_u@10$ with an harmonic mean: $F_{ru} = 2 \times \frac{P_r@10 \times P_u@10}{P_r@10 + P_u@10}$. 
%
%When reranking runs, we risk to bring to the top of the rankings documents that were not assessed when the task took place. For this reason, we inform the average percentage of documents that were not assessed in the top 10 documents of each run (Unj@10) and we inform $P_r@10*$, $P_u@10*$ and $F_{ru}*$, modified measures that excluding all unassessed documents for the ranking list before evaluation.
%These last three metrics showcase the potential of a run if it had been included in the pool set.


% MOVE TO ONLINE APPENDIX:
%%%\begin{figure}
%%%    \caption{\todo{Feature selection experiment: different ML models were evaluated with various number of features in a 10 Cross Validation experiment. Top feature selection were made with mutual info algo. With 10 features, the selection made in our table 3 worked better than the selection automatically made mutual info. However, a larger number of features, at least 200, reached the best results. That explains why our L2R method worked better using all features than using only the selected set of 10 top features. We can discuss it in the text and link a better version of this figure in the online page. THe experiments made here can be improved if I run everything with XGB instead of LR. I am planning to do it tomorrow when I wake up.}}
%%%\includegraphics[width=0.3\textwidth]{graphics/features}
%%%\end{figure}


We start our experiments by showing at the top of Table~\ref{tab:experiments} (indices 1-3) the performance of the top 3 systems in CLEF eHealth 2016 together with a straightforward BM25 baseline run made with Terrier toolkit. Our further experiments will use not only these runs as a comparison base, but modify these runs when necessary.

The first batch of experiments, indices 4-6, consists of re-ranking the top 20 documents based solely on the Dale-Chall index, as it was the metrics with highest correlation with human judgements for this collection.
The topical relevance of the re-ranked resulting lists is immediately hurt in benefit of understandability. Note how $P_r@10$ and $P_r@10^*$ decrease while $P_u@10$ and in special $P_u@10^*$ increase.
The percentage of unassessed documents reach 10\% and, unfortunately, only looking at RBP and uRBP we cannot see the understandability benefits brought by the re-rank method.

The next batch of experiments swaps the Dale-Chall Index by a tree boosting method (aka. XGB or GBDT or GBM) learned using all features of Table~\ref{tab:doc_features}\footnote{Experiments with various Machine Learning methods and feature selection schemes were performed as well and are available in the online appendix. The results of other methods are similar. When using the set of top 10 features for each group from Table~\ref{tab:top_corr_metrics} results are always better than using an automatic method such as Chi Square to select 10 features to use.}.
While the higher correlation of Dale-Chall was XXX (as reported in table Y), correlations using XGB reached YYY, the closest method to the human agreement in this task.

Indices 7-15:
We experiment with reranking the top 15 (more more conservative method), 20 and 50 (a more heterodox method). We clearly see the trade-off between understandability and relevance when we increase from top 15 to top 50: while the $P_r@10$ falls 5 points, $P_u@10$ increases 10. Nevertheless, $F_{ru}*$ shows that more conservative methods provide the best trade-offs.
When comparing the use of Dale-Chall and Linear Regression reranking the same number of documents (indices 13-16), we notice that LR results for $P_u@10*$ are always superior to Dale-Chall, meaning it does a better job when bringing the easier documents to the top of the rank, but that not necessarily means the final rank is better, as we can see comparing the results for $F_{ru}*$.

Our third batch of experiments, indices 20-31, aims to combine the good ranking for topicality coming from the original rank list with the best rankings for understandability, coming from the LR reranking.
For that, we use the Reciprocal Rank Fusion (RRF) method, which combines runs on the rank of documents instead of scores~\cite{cormack09}. Reranking with LR and combining runs with RRF allows to obtain the best trade-offs so far, with $F_{ru}$ of the best method in CLEF being improved from 39.05 to 39.68, with comes from a very small decrease in $P_u@10*$ (from 31.70 to 31.60) and a 4.9\% increase in $P_u@10*$ (from 50.83 to 53.30).

Our final batch of experiments, indices 32-34, are based on learning-to-rank. We decided to use the pairwise implementation of the gradient boosting approach from the XGBoost framework~\cite{chen16} due to its recent good results in many machine learning tasks \todo{actually, I should use XGBoost method instead of LR}. Evaluating other learning to rank methods and frameworks are left as future work.
We evaluated three different settings based on the plain BM25 run shown in index 4, each one with a different combination of feature set and relevance labels:
\begin{itemize}
    \item Index 32: we used as relevance labels only the topical labels and as feature set the score of different information retrieval models (BM25, Dirichlet LM and PL2). This represents the typical use of learning to rank aiming to optimize only the topical relevance results. Unfortunately that was not the case and not even the $P_r@10*$ was improved.
    \item Index 33: we used as relevance labels the linear combination of topical relevance and understandability. Additionally to the IR features, we used the 10 features correspondent to the best features according to Kendall tau in Table~\ref{tab:top_corr_metrics}. Results show that this resulted in higher $F_{ur}*$ when compared to its correspondent approaches (indices 4, 8, 12, 16, 19, 23, 27 and 31) due to a high increase in $P_r@10*$, which reveals understandability features improving topicability.
    \item Index 34: On top of our previous run, we included here all features shown in Table~\ref{tab:doc_features}. Note that despite the very high number of unassessed documents, this run shows improvements even in $P_r@10$ obtaining the same results as the third best run in the campaign (ECNU). In its turn, the $P_{r}*$ which ignores the unassessed documents is 8.4\% higher than the best system, while $F_{ru}*$ is 5.2\% higher.
\end{itemize}


\input{tables/tab_experiments}


