\section{Integrating Understandability into Retrieval}
\label{sec:experiments}

Next, we investigate how understandability estimations could be used and integrated into retrieval methods to increase the quality of search result. To this aim, we first investigated re-ranking search results from an initial run purely based on understandability estimations. 

Specifically, we considered three retrieval methods of differing quality for the initial retrieval. These included the best two runs submitted to each CLEF task (2015 and 2016), and a plain BM25 baseline (default Terrier parameters: $b=0.75$ and $k1=1.2$). As understandability estimators we used  eXtreme Gradient Boosting (XGB)\footnote{For assessed documents, we used 10-fold cross validation, so that we trained XGB on 90\% of the data, and used its predictions for the remaining 10\%. For unassessed documents, we trained XGB on all assessed data, and applied this model to generate predictions.} regressor, as well as SMOG for CLEF 2015 and Dale-Chall for CLEF 2016. These were the best performing approaches from Section~\ref{sec:beyond_readability}.

If all the search results from a run were considered, then such a re-ranking method may place at early ranks Web pages highly likely to be understandable, but possibly less likely to be topically relevant. To balance relevance and understandability, we only re-ranked the first $k$ documents. We explored $k = 15, 20, 50$. Because evaluation was performed with respect to the first $n=10$ rank positions, the setting $k=15$ provided a conservative re-ranking of search results; while, $k=50$ provided a more speculative re-ranking approach. Results are presented in Section~\ref{results:reranking}.

As an alternative to the previous two-step ranking strategy for combining topical relevance and understandability, we explored the fusion of two search results lists separately obtained for relevance and understandability. For this, we used the Reciprocal Rank Fusion (RRF) method~\cite{cormack09}, which has proven effective for combining two lists of search results based on their documents \textit{ranks}, rather than scores. This approach was selected above score-based fusion methods because of the different scoring strategies and distributions employed when scoring for relevance compared to for understandability. For topical relevance, we used, separately, the three methods used for re-ranking (\todo{XXXbest runsXXX} for CLEF2015, GUIR and ECNU for CLEF 2016, and BM25 for both collections). For understandability, we used, separately, the estimations from SMOG/Dale Chall and XGB. Also for this approach we studied limiting the ranking of results to be considered by the methods across the cut-offs $k=15, 20, 50$. Results are presented in Section~\ref{results:fusion}.

Finally, we considered a third alternative to combine topical relevance and understandability: using learning to rank with features derived from the retrieval method and the understandability estimators.
In this experiment, we explored different label attribution and feature sets, keeping the same learning to rank algorithm, again XGB. We first experiment with settings made to optimise only relevance (we consider different Information Retrieval models (BM25, L as feature

\todo{need more details: In the LTR we need to mention the combination of labels to make up the final document label which will be used by the method. Also, we need to say which method we used (pairwise tree boosting) and which are the features for each run.... Note, we only performed learning to rank on the BM25 baseline, because we do not have access to the retrieval features used in the CLEF submissions.}. Results are presented in Section~\ref{results:ltr}.

\input{tables/tab_experiments}

\vspace{-10pt}
\section{Evaluation of Understandability Retrieval}
\label{sec:results}

We report the results obtained when experimenting with the retrieval methods described above in Table~\ref{tab:experiments}. We report only the results for CLEF 2016 (thus we only report Dale Chall) for brevity; those on CLEF 2015 exhibited similar trends and are included in the online appendix. The effectiveness of the top two submission to CLEF 2016 and the BM25 baseline are reported at indices 1-3 of the table. Statistical significant differences compared to the best participating run, GUIR, are reported with $\diamond$; differences between the original run (indices 1-3) and its modification are reported with $\dagger$ (paired, two-tail t-test, $p<0.05$). Note that the baseline BM25 is significantly worse than GUIR across all measures. \todo{JP: actually note that the dagger IS NOT a comparison with ECNU, rather it is comparing a modified run with the original version} 

%We start our experiments by showing at the top of Table~\ref{tab:experiments} (indices 1-3) the performance of the top 3 systems in CLEF eHealth 2016 together with a straightforward BM25 baseline run made with Terrier toolkit. Our further experiments will use not only these runs as a comparison base, but modify these runs when necessary.

\subsection{Re-ranking}
\label{results:reranking}

Indices 4-6 of Table~\ref{tab:experiments} report on our experiments re-ranking documents from runs listed at indices 1-3, up to rank 20, according to their Dale-Chall index.
We find that the topical relevance of the re-ranked runs, represented by $NDCG_r$ significantly decreases when compared to their original runs. For example, the $NDCG_r$ of the re-ranked Dale-Chall version of BM25 decreased from 22.02 to 18.53. However, the re-ranked results were significantly more understandable, as reported by their $NDCG_u$. 
Note a limitation of RBP or uRBP metrics to reveal such trend, as both relevance and understandability are tied together.
% As the number of unassessed documents increases ($Una@10$)

Next, we swap the Dale-Chall Index by XGB regressor trained using all features of Table~\ref{tab:doc_features}\footnote{Experiments with various Machine Learning methods and feature selection schemes were performed as well and are available in the online appendix. The results of other methods are similar.}. % When using the set of top 10 features for each group from Table~\ref{tab:top_corr_metrics} results are always better than using an automatic method such as Chi Square to select 10 features to use.}.
%While the higher correlation of Dale-Chall was XXX (as reported in table Y), correlations using XGB reached YYY, the closest method to the human agreement in this task.
We experiment with re-ranking at cut-off 15 (more more conservative method), 20 and 50 (a more heterodox method). 
A similar understandability-relevance trade-off is seen when using a machine learning regressor in place of the Dale-Chall index.
Note that, for the same cut-off value\footnote{We shown here only Dale-Chall cut-off of 20 for brevity.}, the machine learning method consistently yields more understandable results (higher $NDCG_u$). 

\subsection{Rank Fusion}
\label{results:fusion}

Next, from the index 16 to 24, we report the results of our approach to automatically combine topical relevance and understandability through rank fusion.
Once more, allowing re-ranks at higher cut-offs result in larger gains in terms of understandability, but in small losses in terms of topical relevance.
However, the combination of understandability and relevance, $H_ru$, is more stable now than for the previous re-rank methods. 
For example, compare the $H_{ru}^*$ of 13 (21.97) and 22 (24.34) with the original run at index 1 (24.77).

\subsection{Learning to Rank}
\label{results:ltr}

Last, we evaluate our experiments with learning to rank. 


Our final batch of experiments, indices 32-34, are based on learning-to-rank. We decided to use the pairwise implementation of the gradient boosting approach from the XGBoost framework~\cite{chen16} due to its recent good results in many machine learning tasks \todo{actually, I should use XGBoost method instead of LR}. Evaluating other learning to rank methods and frameworks are left as future work.
We evaluated three different settings based on the plain BM25 run shown in index 4, each one with a different combination of feature set and relevance labels:
\begin{itemize}
    \item Index 32: we used as relevance labels only the topical labels and as feature set the score of different information retrieval models (BM25, Dirichlet LM and PL2). This represents the typical use of learning to rank aiming to optimize only the topical relevance results. Unfortunately that was not the case and not even the $P_r@10*$ was improved.
    \item Index 33: we used as relevance labels the linear combination of topical relevance and understandability. Additionally to the IR features, we used the 10 features correspondent to the best features according to Kendall tau in Table~\ref{tab:top_corr_metrics}. Results show that this resulted in higher $F_{ur}*$ when compared to its correspondent approaches (indices 4, 8, 12, 16, 19, 23, 27 and 31) due to a high increase in $P_r@10*$, which reveals understandability features improving topicability.
    \item Index 34: On top of our previous run, we included here all features shown in Table~\ref{tab:doc_features}. Note that despite the very high number of unassessed documents, this run shows improvements even in $P_r@10$ obtaining the same results as the third best run in the campaign (ECNU). In its turn, the $P_{r}*$ which ignores the unassessed documents is 8.4\% higher than the best system, while $F_{ru}*$ is 5.2\% higher.
\end{itemize}





