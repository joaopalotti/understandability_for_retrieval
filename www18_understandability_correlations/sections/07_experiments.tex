\section{Integrating Understandability into Retrieval}
\label{sec:experiments}

Next, we investigate how understandability estimations could be used and integrated into retrieval methods to increase the quality of search result. To this aim, we first investigated re-ranking search results from an initial run purely based on understandability estimations. 

Specifically, we considered three retrieval methods of differing quality for the initial retrieval. These included the best two runs submitted to each CLEF task (2015 and 2016), and a plain BM25 baseline (default Terrier parameters: $b=0.75$ and $k1=1.2$). As understandability estimators we used  eXtreme Gradient Boosting (XGB)\footnote{For assessed documents, we used 10-fold cross validation, so that we trained XGB on 90\% of the data, and used its predictions for the remaining 10\%. For unassessed documents, we trained XGB on all assessed data, and applied this model to generate predictions.} regressor, as well as SMOG for CLEF 2015 and Dale-Chall for CLEF 2016. These were the best performing approaches from Section~\ref{sec:beyond_readability}.

If all the search results from a run were considered, then such a re-ranking method may place at early ranks Web pages highly likely to be understandable, but possibly less likely to be topically relevant. To balance relevance and understandability, we only re-ranked the first $k$ documents. We explored $k = 15, 20, 50$. Because evaluation was performed with respect to the first $n=10$ rank positions, the setting $k=15$ provided a conservative re-ranking of search results; while, $k=50$ provided a more speculative re-ranking approach. Results are presented in Section~\ref{results:reranking}.

As an alternative to the previous two-step ranking strategy for combining topical relevance and understandability, we explored the fusion of two search results lists separately obtained for relevance and understandability. For this, we used the Reciprocal Rank Fusion (RRF) method~\cite{cormack09}, which has proven effective for combining two lists of search results based on their documents \textit{ranks}, rather than scores. This approach was selected above score-based fusion methods because of the different scoring strategies and distributions employed when scoring for relevance compared to for understandability. For topical relevance, we used, separately, the three methods used for re-ranking (ECNU~\cite{song15} and KISTI~\cite{oh15} for CLEF2015, GUIR~\cite{soldaini16} and ECNU~\cite{song16} for CLEF 2016, and BM25 for both collections). For understandability, we used, separately, the estimations from SMOG/Dale Chall and XGB. Also for this approach we studied limiting the ranking of results to be considered by the methods across the cut-offs $k=15, 20, 50$. Results are presented in Section~\ref{results:fusion}.

Finally, we considered a third alternative to combine topical relevance and understandability: using learning to rank with features derived from retrieval methods and the understandability estimators.
With CLEF 2016 collection, we explored five combinations of label attribution and feature sets, keeping the same pairwise learning to rank algorithm based on tree boosting (XGB).
These combinations are listed in Table~\ref{tab:ltr}, with $R$ being the topical relevance of documents and $U$ their understandability assessments. 

\input{tables/tab_ltr}

\input{tables/tab_experiments}

\vspace{-10pt}
\section{Evaluation of Understandability Retrieval}
\label{sec:results}

We report the results obtained when experimenting with the retrieval methods described above in Table~\ref{tab:experiments}. We report only the results for CLEF 2016 (thus we only report Dale Chall) for brevity; those on CLEF 2015 exhibited similar trends and are included in the online appendix. The effectiveness of the top two submission to CLEF 2016 and the BM25 baseline are reported at indices 1-3 of the table. Statistical significant differences compared to the best participating run, GUIR, are reported with $\diamond$; differences between the original run (indices 1-3) and its modification are reported with $\dagger$ (paired, two-tail t-test, $p<0.05$). Note that the baseline BM25 is significantly worse than GUIR across all measures. 

%We start our experiments by showing at the top of Table~\ref{tab:experiments} (indices 1-3) the performance of the top 3 systems in CLEF eHealth 2016 together with a straightforward BM25 baseline run made with Terrier toolkit. Our further experiments will use not only these runs as a comparison base, but modify these runs when necessary.

\subsection{Re-ranking}
\label{results:reranking}

Indices 4-6 of Table~\ref{tab:experiments} report on the experiments re-ranking the top 20 documents from runs listed at indices 1-3. This re-rank was made based on the Dale-Chall index of each document calculated using the preprocessing combination of Boilerpipe ForcePeriod (best one according to Pearson correlation - Table~\ref{tab:top_corr_metrics}).
We find that the topical relevance of the re-ranked runs, represented by $RBP_r$ significantly decreases when compared to their original runs. For example, the $RBP_r$ of the re-ranked Dale-Chall version of BM25 decreased from 25.28 to 22.19. However, the re-ranked results were significantly more understandable, as reported by their $RBP_u$: from 42.08 to 46.99. 
Note a limitation of uRBP metrics to reveal such trend, as both relevance and understandability are tied together in one single score.

Next, we swap the Dale-Chall Index by XGB regressor trained using all features listed in Table~\ref{tab:doc_features}\footnote{Experiments with various Machine Learning methods and feature selection schemes were performed as well and are available in the online appendix. The results of other methods are similar.}. % When using the set of top 10 features for each group from Table~\ref{tab:top_corr_metrics} results are always better than using an automatic method such as Chi Square to select 10 features to use.}.
%While the higher correlation of Dale-Chall was XXX (as reported in table Y), correlations using XGB reached YYY, the closest method to the human agreement in this task.
We experiment with re-ranking at cut-off 15 (a more conservative method), 20 and 50 (a more heterodox method). 
A similar understandability-relevance trade-off is seen when using a machine learning regressor in place of the Dale-Chall index.
Note that, for the same cut-off value\footnote{We show here only Dale-Chall cut-off of 20 for brevity.}, the machine learning method consistently yields more understandable results (higher $RBP_u$ and $RBP_u^*$). 

\subsection{Rank Fusion}
\label{results:fusion}

Next, from the index 16 to 24, we report the results of our approach to automatically combine topical relevance and understandability through rank fusion.
Once more, allowing re-ranks at higher cut-offs result in larger gains in terms of understandability, but in small losses in terms of topical relevance.
However, the combination of understandability and relevance, $H_{ru}$, is more stable now than the previous re-rank methods. 
For example, compare the $H_{ru}^*$ of methods that re-rank the top 50 documents per topic, XGB at index 13 (21.97) and RRF at index 22 (24.34), with the original run at index 1 (24.77).

\subsection{Learning to Rank}
\label{results:ltr}

Last, we evaluate the experiments with learning to rank, indices 25-29. Note that we do not impose a maximum rank at which the learning-to-rank methods can re-rank, therefore we focus the following analyse on the metrics that ignore unassessed documents (the ones signed with a $^*$).

Combo 1 (index 25), which only uses Information Retrieval features\footnote{We devised 24 Information Retrieval features using the Terrier framework. The score of various retrieval models were extracted from a multi-field index composed of title, body and whole document.}, is trained only on topical relevance labels. Although simple, this is a typical learning to rank setting.
No significant difference was found between this setting and the original BM baseline for any metric.

Compared to Combo 1, Combo 2 (index 26) includes the understandability features listed in Table~\ref{tab:doc_features}. Although not statistically different from the baseline, the inclusion of understandability features was as beneficial to the understandability relevance as for the topical relevance.

Combo 3 explores the use of understandability assessments of CLEF 16 in a straightforward and effective manner. As the goal is to retrieve very easy-to-read documents (those with understandability label 0), a penalty is proportionally given to documents as far as their understandability score was further from the target. A document with understandability label 100 would receive label 0, while another with label of 50 would have only half of its topical relevance score.

Combos 4 and 5 were devised based on a pre-defined understandability threshold $U$. While Combo 4 takes in consideration only documents that are easy-to-read (understandability label $\le$ $U$), Combo 5 considers all documents, but boosts the relevance score of easy-to-read documents. In this work $U=40$, mimicking the evaluation formulas proposed in Section~\ref{sec:data}. While Combo 4 reaches the highest understandability score for the learning-to-rank approaches ($RBP_u^{*}$ of 50.06), it fails to retrieve topically relevant documents ($RBP_r^{*}$ of only 22.20). In turn, Combo 5 reaches the best understandability-relevance trade-off ($H_{ru}^{*}$ of 29.20), being able to largely increase the topical relevance from the BM 25 baseline in which it is based ($RBP_r^*$ from 26.01 to 32.60 - an 25\% increase), while also improving understandability ($RBP_u^*$ from 43.89 to 45.87 - a 4\% increase). Note that Combo 5 is also significatively better than the best participant run of CLEF
2016 for both $RBP_r^{*}$ (15\% increase) and $H_ru^{*}$ (13\% increase).




