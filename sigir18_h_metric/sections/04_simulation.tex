
\section{Comparing frameworks Through System Simulations}
\label{sec:simulations}

%While Section~\ref{sec:understandability_metrics} revised uRBP, Zuccon's modification of the gain-discount framework to incorporate understandability along with topicality in one single formula,
%Section~\ref{sec:extension} proposed an alternative approach which calculates an score for each dimension first and then combine these scores into a unique value with an harmonic mean.
%In this section, we aim to evaluate uRBP and $H_{RBP}$, our proposed alternative, comparing each other with simulations.
%\mytodo{Start by a reason to use simulations in this work}

The aim of this section is to understand the behaviour of each evaluation framework when facing different types of Information Retrieval systems.
In order to have a fine-grained control over our experiments, we employed synthetic/simulated systems.
This way, we know exactly what to expect from each system and evaluation metric before calculating any score.
\mytodo{Find some other paper that made their evaluation with simulated systems}.

Through the expected results of the simulated systems, we demonstrate how the proposed two-step framework, $H$ (Section~\ref{sec:extension}), fixes the main drawback found in Zuccon's modification of the gain-discount framework (Section~\ref{sec:understandability_metrics}). 

Without loss of generality, we set our further experiments in the same context as UBIRE was proposed: to evaluate both topicality and understandability of documents in the context of consumer health search. This decision helps the flow of this paper, but by no means these frameworks are limited to evaluate this specific dimensions. 

In our further experiments, we have full control over both topical and understandability relevance of documents. 
The behaviour of the proposed evaluation metrics is investigated when we slightly increase/decrease the amount of topically relevant documents or when we increase/decrease the expected reading difficult of the retrieved documents. 
For that, we generated simulated runs following the this two-phases procedure:

\begin{enumerate}
\item \textbf{Topicality Phase:} In this phase, we exclusively control the amount of topical relevant documents in a simulated run. That is done with a simple random variable $T$, $0 \le T \le 1$. 
We constructed a simulated run by drawing a real number $N$, $0 \le N \le 1$, for each position $i$ in a ranking. If $N \le T$, we mark the document at position $i$ as relevant, otherwise, we mark that document as not relevant. It is expected that a run generated with $T=0.1$ would have 10\% of the documents assessed as relevant and 90\% as non-relevant, while a run with $T=0.5$ will have as many relevant as non-relevant documents. 

\item \textbf{Understandability Phase:}  Once the previous phase is done, we start this one. Here, we control the level of understandability of the results. In order to create and control the randomness of our simulated systems, we generate understandability labels with a Gaussian distribution with pre-defined mean $\mu$ and variance $\sigma$. 
As previously done in consumer health search collections~\cite{clefIR16,clefIR17}, we force the understandability labels generated to be in the interval from 0 to 100. 
We fixed a relatively large variance, $\sigma=40$, to mimic results of previous collections in which the understandability labels have a large variance \cite{clefIR16}, and we vary the mean $\mu$ of the Gaussian from 0 to 100. Figure~\ref{fig:gaussians} shows the expected label distribution for $\mu=20, 50, 80$, i.e., $\mathcal{N}(20, 40)$, $\mathcal{N}(50, 80)$ and $\mathcal{N}(80, 40)$.
In Figure~\ref{fig:gaussians} we also include the threshold U used to compute $RBP_u$ (Section~\ref{sec:extension}).

\end{enumerate}

\begin{figure}[t!]
  \centering
   \includegraphics[width=0.45\textwidth]{figs/gaussians}
    \caption{Gaussian distribution for different $\mu$ values. A distribution with a higher $\mu$ generates higher understandability labels simulating that documents harder to read were retrieved. In the experiments in this paper, only documents with an understandability label lower than 40 are considered to easy-to-read, and therefore relevant in the understandability dimension. This understandability threshold is shown with a traced line.}
  \label{fig:gaussians}
\end{figure}

In total, we generated 1,000 runs for each topical relevance level (topicality phase) and $mu$ value (understandability phase). 

We calculated the $uRBP$ score from UBIRE framework and the $H_{RBP}$ score from our proposed framework for each simulated system.
The average result of selected scenarios is shown in Table~\ref{tab:simulations}.

Each row of Table~\ref{tab:simulations} shows the results of simulations with different values for $T$, i.e., different expected number of relevant documents retrieved.
We varied the $\mu$ parameter of the Gaussian distributions employed to create the understandability labels and show the results for $\mu = 50, 40, 30$.
A smaller $\mu$ means that more understandable documents are retrieved.
In total, the results of fifteen experiments are shown in Table~\ref{tab:simulations}.

Table~\ref{tab:simulations} shows that as the expected number of relevant documents ($T$) is increased, $RBP$ increases.
Likewise, $uRBP$ increases, as it is bounded with topical relevance.
In turn, increasing $T$ has no effect on $RBP_u$, but increases $H_{RBP}$, as it also directly depends on $RBP$.

Increasing the number of understandable documents retrieved (i.e., decreasing $\mu$), $RBP$ stays constant, as it does not measure how understandable documents are.
In turn, $uRBP$, $RBP_u$ and $H_{RBP}$ increase.

Those are the expected behaviour of the aforementioned metrics and only shows that our simulated runs produce sane results.

The trick part that we aim to show with these experiments is illustrated by the cells colored in blue and yellow in Table~\ref{tab:simulations}.
We modeling and experimenting with a retrieval system, a modification that aims to increase the understandability of the documents retrieved by the system might hurt the topical relevance of the documents retrieved by the system.
This situation could be translated in an initial system with results shown in blue in Table~\ref{tab:simulations} (i.e., $T=0.6$ and $\mathcal{N}(40,40)$) 
being modified into the system with results shown in yellow ($T=0.5$ and $\mathcal{N}(30,40)$).

If the person doing the system modification considers only the $RBP$ and the $uRBP$, she would definitely discard the new systems (marked in yellow in Table~\ref{tab:simulations}) as both $RBP$ and $uRBP$ decreased ($RBP$ decreased from 56 to 47 and $uRBP$ decreases from 32 to 30). Based only on $RBP$ and $uRBP$ from UBIRE framework, it is not possible to notice the gains in the understandability dimension.

However, when separately measuring $RBP_t$ (which is the same as $RBP$) and $RBP_u$, we can understand better what is happening with each dimension: $RBP_t$ decreases from 56 to 47, but $RBP_u$ increases from 49 to 60, i.e., the trade-off between the two dimensions is clearly shown.
Finally, $H_{RBP}$, which is instantiated to give the same importance to both dimensions, shows that, although very different, both system are equivalent. 
In a real system, if document topicality is more important than document understandability, the weights of each dimension can be tweaked in the harmonic mean formula, exactly as precision and recall are balanced in the $F$ formula.

\input{tables/tab_simulations}
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

