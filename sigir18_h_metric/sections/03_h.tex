
\section{A new Framework for Multi-Dimension IR Evaluation}
\label{sec:extension}

%A drawback of Zuccon's framework is that dimensions are combined into a unique evaluation score.
UBIRE framework for multi-dimension evaluation combines the results of all dimensions in \textbf{one} single step, providing a unique evaluation score~\cite{zuccon14,zuccon16}.
While this unique score makes the comparison of different systems easy and direct, we argue that the combination of dimensions, as done in UBIRE, was premature.
We propose instead, to separate the evaluation of each dimension, calculating first a unique value for each dimension separately and then combining scores into a unique effectiveness metric.
Our proposal thus requires the possibility of evaluating each dimension separately and the existence of a function to combine the output of each dimension.

The separated evaluation of each dimension is trivial, it consists in applying a pre-defined evaluation metric, $m$ (e.g. RBP), to each dimension $d \in D$.
%Each other dimension $D_i$ can be evaluated in the same fashion that $P(D_i|d@k)$ is assessed in UBIRE~\cite{zuccon16}.
%For example, the binary calculation of the understandability metric $P(U|d@k)$ returns 1 if $P(U|d@k) \geq th_U$ and 0 otherwise, where $th_U$ is a threshold on the assessments of understandability. 

In order to combine the outputs of each dimension, we propose the use of the weighted harmonic mean, which is already successfully employed to balance recall and precision in the widely used $F$ metric.


To define the weighted harmonic mean proposed, we need the aforementioned evaluation metric $m$, which will be applied to evaluate a list of labeled documents $l_d$ (i.e., $m(l_d)$).
Also, we need a set of weights for each dimension, which control the importance of each dimension.
We define the weighted harmonic mean $H(m)$ as:

\begin{equation}
    H_m  = \left( \frac{\sum\limits_{d=1}^D w_d \cdot m(l_d)^{-1}}{\sum\limits_{d=1}^D w_d} \right)^{-1}
          = \frac{\sum\limits_{i=1}^D w_i}{\sum\limits_{i=1}^D \frac{w_i}{m(l_d)}}
\label{eq:H}
\end{equation}

%
%\begin{equation}
%
%    H(f) = \Bigg(\frac{\sum\limits_{d=1}^{D} O_d^{-1} }{D} \Bigg)^{-1} = \Bigg(\frac{D}{\sum\limits_{d=1}^{D} O_d^{-1} } \Bigg) = \Bigg(\frac{ D \times \prod\limits_{d=1}^{D} O_d}{\sum\limits_{d=1}^{D} O_d } \Bigg)
%
%\label{eq:H}
%\end{equation}


%
Without loss of generality, we instantiate $m = RBP$ and define the following modification of RBP~\cite{moffat08} for each dimension:
%
\begin{itemize}[leftmargin=*]
	\item $RBP_r(\rho)$: uses the relevance assessments for the top $n$ search results (i.e. this is the regular RBP). 
%	
    \item $RBP_u(\rho)$: uses the understandability assessments for the top $n$ search results. In our experiments, we regard a document as understandable if its assessed understandability score was smaller than a threshold $U$ (we arbitrary choose $U = 40$ in our simulated experiments).
\end{itemize}

We can rewrite Equation~\ref{eq:H} with the definitions above and same weights to both dimensions as:
%
\begin{equation}
 H_{RBP(\rho)} = 2 \cdot \frac{RBP_r(\rho) \cdot RBP_u(\rho)}{RBP_r(\rho) + RBP_u(\rho)}
\label{eq:Hrbp}
\end{equation}

%Note that the $H_{RBP(\rho)}$ metric defined above gives the same weight to both topicality and understandability, just as the $F_1$ score gives the same weight to precision and recall. 
%On our framework, likewise the $F$ metric, other weights can be easily defined depending on the experiment setting.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

