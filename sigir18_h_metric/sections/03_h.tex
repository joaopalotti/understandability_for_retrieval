
\section{A new Framework for Multi-Dimension IR Evaluation}
\label{sec:extension}

%A drawback of Zuccon's framework is that dimensions are combined into a unique evaluation score.
A limitation of the UBIRE framework is that it combines the gains contributed by each dimension of relevance in \textbf{one} single step, providing a unique evaluation score~\cite{zuccon14,zuccon16}.
While this allows for the comparison of systems, it does not permit to understand the contribution each dimension had on the evaluation measure. 
\todo{Here I would make an example.}
To overcome this limitation, we aim to create a measure which, while still allowing the modelling of multidimensional relevance, is of easy interpretation and for which it is straightforward to track the contribution each relevance dimension had on the final effectiveness score. This is achieved by separating the evaluation of each dimension such that a value for each dimension is calculated separately with respect to its gain and discount, and then these are combined into a unique effectiveness measure. Note that we assume that it is possible to evaluate each measure separately: while this is akin to the compositionality assumptions in UBIRE, if that failed, UBIRE would use mixture models to compute the related probabilities, while the proposed measure would be instead likely undefined. 

The evaluation of each relevance dimension separately is trivial, as it consists in applying the discount and gain function of the underlying evaluation measure, e.g. RBP, to each relevance dimension $\delta \in \mathcal{D}$, where the gains are those associated with the criteria for that specific dimension. 


%While this unique score makes the comparison of different systems easy and direct, we argue that the combination of dimensions, as done in UBIRE, was premature.
%We propose instead, to separate the evaluation of each dimension, calculating first a unique value for each dimension separately and then combining scores into a unique effectiveness metric.
%Our proposal thus requires the possibility of evaluating each dimension separately and the existence of a function to combine the output of each dimension.

%The separated evaluation of each dimension is trivial, it consists in applying a pre-defined evaluation metric, $m$ (e.g. RBP), to each dimension $d \in D$.
%Each other dimension $D_i$ can be evaluated in the same fashion that $P(D_i|d@k)$ is assessed in UBIRE~\cite{zuccon16}.
%For example, the binary calculation of the understandability metric $P(U|d@k)$ returns 1 if $P(U|d@k) \geq th_U$ and 0 otherwise, where $th_U$ is a threshold on the assessments of understandability. 

%\todo{Briefly, mention why we did not use another function like linear combination instead of harmonic mean.}
%In order to combine the outputs of each relevance dimension, we use weighted harmonic mean (akin to its use to combine recall and precision in the widely used $F$-measure). 
While the outputs of each relevance dimension could be combined with a linear or geometric combination of values, we opt to use the weighted harmonic mean, as it is particularly sensitive to a single lower-than average value. The same intuition is used to combine recall and precision in the widely used $F$-measure. 
Given a (discount-gain) evaluation measure $\mathcal{M}$, we apply the measure to evaluate a list of  documents $l_\delta$ which have been labeled with respect to dimension $\delta$ (i.e., we compute $\mathcal{M}(l_\delta)$). Then, to compute the proposed measure $H_\mathcal{M}$, we combine all $\mathcal{M}(l_\delta)$ for each relevance dimension using the harmonic mean, where each dimension is weighted according to a preferential weight $w_\delta$ assigned to each dimension; formally:

%To define the weighted harmonic mean proposed, we need the aforementioned evaluation metric $m$, which will be applied to evaluate a list of labeled documents $l_d$ (i.e., $m(l_d)$).
%Also, we need a set of weights for each dimension, which control the importance of each dimension.
%We define the weighted harmonic mean $H(m)$ as:

\begin{equation}
    H_\mathcal{M}  = \left( \frac{\sum\limits_{\delta=1}^D w_\delta \cdot \mathcal{M}(l_\delta)^{-1}}{\sum\limits_{\delta=1}^D w_\delta} \right)^{-1}
          = \frac{\sum\limits_{\delta=1}^D w_\delta}{\sum\limits_{\delta=1}^D \frac{w_\delta}{\mathcal{M}(l_\delta)}}
\label{eq:H}
\end{equation}

%
%\begin{equation}
%
%    H(f) = \Bigg(\frac{\sum\limits_{d=1}^{D} O_d^{-1} }{D} \Bigg)^{-1} = \Bigg(\frac{D}{\sum\limits_{d=1}^{D} O_d^{-1} } \Bigg) = \Bigg(\frac{ D \times \prod\limits_{d=1}^{D} O_d}{\sum\limits_{d=1}^{D} O_d } \Bigg)
%
%\label{eq:H}
%\end{equation}


%
Without loss of generality, we instantiate $\mathcal{M} = RBP$ and define the following modification of RBP~\cite{moffat08} for each dimension:
%
\begin{itemize}[leftmargin=*]
	\item $RBP_t(\rho)$: uses binary topicality assessments (i.e. the usual RBP). 
%	
    \item $RBP_u(\rho)$: uses understandability assessments. In the experiments below, we regard a document as understandable if its assessed understandability score was smaller than a threshold $U$ (we arbitrary choose $U = 40$ based on the distribution of understandability assessments made at CLEF eHealth 2016~\cite{clefIR16}).
\end{itemize}

Thus Equation~\ref{eq:H} becomes (we assumed $w_t = w_u$):
%
\begin{equation}
 H_{RBP(\rho)} = 2 \cdot \frac{RBP_t(\rho) \cdot RBP_u(\rho)}{RBP_t(\rho) + RBP_u(\rho)}
\label{eq:Hrbp}
\end{equation}

%Note that the $H_{RBP(\rho)}$ metric defined above gives the same weight to both topicality and understandability, just as the $F_1$ score gives the same weight to precision and recall. 
%On our framework, likewise the $F$ metric, other weights can be easily defined depending on the experiment setting.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

