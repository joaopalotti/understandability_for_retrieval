
\mytodo{The content that follows should go to another paper/chapter. The goal is to understand and decide which metric to use to evaluate understandability with shallow pools.}
\section{Evaluation Metrics for Shallow Pools}
\label{sec:shallow}

Assessments made in CLEF eHealth 2015 and 2016 were costly, i.e., organizers had to hire physicians, nursers or medical students to assess the relevance of documents. These specialized assessments are costlier than regular ones. Thus, due to budged limitations, assessment pools were limited. 
Limiting assessments is not a issue for the systems that are participating in the CLEF eHealth campaigns, but it might affect further system's evaluations, specially for systems that are very different from the ones that participated in the campaigns.
This is the case of the learning to rank models that are presented \mytodo{in Chapter XXX}, which rankings contain many unassessed documents. 

We present two solutions to overcome this limitation: 

\begin{enumerate}[leftmargin=*]

\item We propose the use of Bpref~\cite{craswell09} for topical assessments and its variant uBpref~\cite{palotti16} for understandability assessments.
Bpref was designed to be robust to missing relevance assessments in a way that it reaches the same experimental outcome with incomplete assessments that well regarded metrics such as the Mean Average Precision (MAP) would with complete assessments~\cite{craswell09}. Given $R$ the number of relevant documents for a topic, $N$ the number of non-relevance documents for a topic, and a list of documents retrieved in increasing rank, which $k$ represents the current rank, Bpref can be defined as:
%
\begin{equation}
%
Bpref = \frac{1}{R} \times \sum_k \Big( 1 - \frac{min(\text{\# of n above k}, R)}{min(N,R)} \Big)
%
\label{eq:RBP}
\end{equation}
%
In turn, uBpref, modified in the spirit of the understandability-biased evaluation framework and based on the same $P(U|d@k)$ described above for uRBP and uRBPgr, can be defined as:
%
\begin{equation}
%
uBpref = \frac{1}{R} \times \sum_k P(U|d@k) \times Big( 1 - \frac{min(\text{\# of n above k}, R) }{min(N,R)} \Big)  
%
\label{eq:RBP}
\end{equation}
%
Bpref considers only documents that have been explicitly assessed with respect to their relevance. uBpref also considers only assessed documents (for relevance and for understandability). The (binary) gain from the relevance status of an assessed document (0: irrelevant, 1: relevant) is multiplied by the graded gain from the understandability assessment (with weights as in uRBPgr).
%
%\item Along with the measures of search effectiveness, we shall also report the number of unassessed documents, the RBP residuals, and the corresponding measures calculated by ignoring unassessed documents, which we superscript an asterisk (\textbf{*}) to their names: $RBP_r@10^*$, $RBP_u@10^*$, $H_{RBP}^*$, etc. 
\item Along with the metrics of search effectiveness, we shall also report the corresponding mtrecis calculated by ignoring unassessed documents, which we represent with a superscripted asterisk (\textbf{$^*$}): $RBP_r@10^*$, $RBP_u@10^*$, $H_{RBP}^*$, etc. 
%
\end{enumerate}
%
In the next section, we investigate the behaviour of these metrics correlating them with each other using the CLEF eHealth 2015 and 2016 datasets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Understanding Metrics for Shallow Pools}
\label{sec:metric_correlations}


We study the relationship between measures through a correlation analysis. % using the participant systems submitted to both CLEF eHealth 2015 and 2016 campaigns.
We employ here the Kendall-$\tau$ rank correlation which yields values closer to 1 when two metrics have a strong positive correlation and closer to 0 if weak or no correlation are found.
In Figure~\ref{fig:rbp_kendall} we show the Kendall-$\tau$ rank correlation for each pair of metrics for CLEF eHealth 2015 (top) and 2016 (bottom) campaigns.
When two metrics have a strong correlation, the participant systems are ranked similarly. 

We start by comparing the correlation of each metric (RBP, uRBP, so on) and its modified version to ignore unassessed documents (RBP$^*$, uRBP$^*$, so on).
Figure~\ref{fig:rbp_kendall} shows that the correlation between RBP and RBP$^*$ is strong in both 2015 (0.73) and 2016 (0.87) data. So it is the correlation between uRBP and uRBP$^*$ (0.78 in 2015 and 0.88 in 2016), uRBPgr and uRBPgr$^*$ (1.0 in both campaigns), and $H_{RBP}$ and $H_{RBP}^*$ (0.73 and 0.82). The correlation between $RBP_u$ and $RBP_u^*$



Findings:
\begin{itemize}
\item Bpref and uBref show a very high correlation in both experiments. That is not good, as they should measure different things.  
\item uRBPgr and uRBPgr* also have an incredibly high correlation.
\item Bpref shows smaller correlation to RBP. This is expected, as they are different measures.
\item $RBP_u$ and $RBP_u*$ have the smaller correlation. This is also expected as they are very different measures.
\item $H_{RBP}$ correlated well with RBP. 
\end{itemize}

I can do the same experiments with simulated runs which I can plan the correlation in advance. 


\begin{figure*}[t!]
  \centering
   \includegraphics[width=0.8\textwidth]{rbp_kendall15}
   \includegraphics[width=0.8\textwidth]{rbp_kendall16}
    \caption{Kendall-$\tau$ correlation of evaluation metrics for CLEF eHealth 2015 (top) and 2016 (bottom)}
  \label{fig:rbp_kendall}
\end{figure*}


The correlation of uRBP, RBP and RBP\_u, Hrbp, might reveal if they are talking about the same stuff or not.



