
\section{Conclusion}
\label{sec:conclusion}

In this paper, we proposed a new framework, called $H$, to evaluate search engines when multidimensional relevance should be considered.
Using both synthetic and real data, we compared $H$ to the understandability-biased information retrieval evaluation framework (UBIRE), which has recently been used  to evaluate search systems in the consumer health search domain. 

Our experiments showed that while $H$ correlated well with UBIRE and that both had an equivalent power to rank and distinguish good systems, $H$ has the advantage of allowing experimenters to easily understand what relevance dimensions is affecting their systems performance, as well as carefully tune the trade-off between topical relevance and other dimensions. While our empirical experiments only considered understandability as additional dimension to relevance, this was done for directly comparing with UBIRE, and by definition $H$ naturally accommodates for an unlimited number of relevance dimensions. An open question is whether $H$ correlates with human preferences and how it compares with UBIRE in this respect. To answer this, future work will consider user-based validation and comparison of the two multidimensional evaluation frameworks. 
