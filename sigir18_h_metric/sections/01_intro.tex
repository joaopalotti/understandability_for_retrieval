\section{Introduction}
%\section{Evaluation Metrics of Understandability in Search Engines}
\label{chp:evaluation_metrics}

%One of the first notions of relevance was straightforward: a document is relevant to a query if the topic of the information retrieved matches the topic of the request.
%This was called \textit{topical relevance} by Eisenberg and Schamber~\cite{eisenberg88} and measured whether a document was ``on the topic'' or ``on the subject''.
%Due to its simplicity and clear definition~\cite{borlund03}, this notion was adopted by most of the modern evaluation tasks, such as TREC\footnote{\url{http://trec.nist.gov}}, CLEF\footnote{\url{http://www.clef-initiative.eu/}} and NTCIR\footnote{\url{http://http://research.nii.ac.jp/ntcir/index-en.html}}, which heavily rely on evaluation metrics that exclusively measure the topical relevance of documents~\cite{voorhees05}.
%Recently extensions were made based on notions of novelty and diversity (e.g.,~\cite{clarke09}), but these new metrics still mainly consider relevance with respect to document topicality.

Research has long established that the notion of relevance in information retrieval is multidimensional\todo{~\cite{}}: the topicality of a document to a query or information need is central to the notion of relevance, but other factors (also called dimensions) that influence the relevance of a document do exist. These include novelty and diversity, timeliness, scope, understandability and trustworthiness, among others~\cite{park93,schamber94}. In the context of consumer health search\footnote{This search task involves common people with no or limited medical knowledge searching for health advice on the web. This task is often carried out in time-sensitive and emotion-pressured circumstances~\cite{}.}, in particular, the relevance dimensions of understandability and information trustworthiness are fundamental~\cite{hersh08}, as for health information to be valuable to the user and allow them to make appropriate health decision it has to be of easy understanding and of correctness, and it is preferred it is delivered through reliable and recognisable sources. It is therefore important to take into account these relevance dimensions, along with topicality, when evaluating the effectiveness of search systems in the context of consumer health search tasks.

%However, document relevance cannot be attributed to just one factor such as topicality, instead, it is multidimensional and situational~\cite{borlund03}.
%Early research~\cite{saracevic75,swanson86,harter92}, reviewed by Borlund divided relevance into two classes: (1) objective or system-based relevance and (2) subjective or human based relevance~\cite{borlund03}. 
%This first class of relevance exclusively refers to the document ``aboutness'' aforementioned and it is context-free, while the second one refers to the subjective factors in both user and documents and it is context dependent.
%A large body of subsequent research was dedicated to identifying the subjective aspects of relevance.
%Park, for example, identified individual's subject knowledge, professional training, and educational background as a user-based influential factor, while scarcity, availability, timeliness, and scope as document-based factors~\cite{park93}.
%Schamber published a compiling and non-exhaustive list of 80 relevance criteria suggested in the literature~\cite{schamber94}.
%%
%In the consumer health search domain, William Hersh highlighted two important factors to be considered by modern search engines~\cite{hersh08}: understandability and information trustworthiness. 
%While the frameworks described in this paper are broad enough to be used with any multidimensional evaluation, we use as example here the consumer health search scenario as early approaches have been developed in this context.

An evaluation framework that integrates understandability into information retrieval evaluation has been recently devised~\cite{zuccon14,zuccon16} and it has been largely adopted to evaluate systems for consumer health search~\cite{clef17}. The framework, doubted \textit{understandability-biased IR evaluation} (UBIRE), builds upon the gain-discount framework of evaluation measures used in information retrieval (belong to this framework measures like normalised Discounted Cumulative Gain (nDCG), Expected Reciprocal Rank (ERR), Rank Biased Precision metric (RBP))~\cite{carterette11} and it uses a discount based on the rank position at which documents are retrieved, and a gain function that integrates contributions from both topicality and understandability (see Section~\ref{sec:understandability_metrics}). The UBIRE framework has been extended to integrate additional relevance dimensions such as trustworthiness~\cite{clef17}: since its extension is straightforward, without loss of generality, we refer to the UBIRE framework as the extended version capable of including in the gain function every dimension of relevance (provided certain assumptions are met). 

%Early efforts to integrate understandability into information retrieval evaluation was made by Zuccon and Koopman~\cite{zuccon14} and enhanced later by Zuccon~\cite{zuccon16}.
%They refer to their framework as \textit{understandability-biased IR evaluation framework} (UBIRE).
%Under UBIRE, metrics such as $uRBP$ and $uRBPgr$ were built upon the Rank Biased Precision metric (RBP)~\cite{moffat08}. These metrics are used in recent evaluation campaigns in the context of CLEF eHealth to combine understandability of documents with topical relevance (and more recently they included a dimension for document trustworthiness~\cite{clef17}.
%$uRBP$, short for understandability-biased RBP and its graded version, $uRBPgr$, merge the relevance and understandability scores of a document into a unique score following the gain-discount framework by Carterette~\cite{carterette11}. We shall review UBIRE framework in Section~\ref{sec:understandability_metrics}. 

A limitation of the approach used to model multi-dimensional relevance in the UBIRE framework is that it is not trivial to identify how the different dimensions of relevance affect the final score returned by measures in this framework. This is because in UBIRE gains produced by documents for each of the considered dimensions of relevance are combined early on in the evaluation measure. This limitation makes the interpretation of evaluation results using UBIRE difficult as it is impossible to determine whether improvements (deterioration) are due to more (less) understandable or more (less) topical documents being retrieved. 

In this work we propose an alternative framework to UBIRE, called the H framework, that overcomes the interpretability limitation of UBIRE, while still enabling the combination of multidimensional relevance evidence when evaluation information retrieval systems (Section~\ref{sec:extension}). Using a small example test case we show the intuitive differences between UBIRE and H and demonstrate how H overcomes UBIRE's limitation (Section~\ref{sec:simulations}). We further empirically compare a specific measure instantiated from the two frameworks to study system ranking correlations across UBIRE and H (Section~\ref{sec:clef}). The results show that \todo{... what do they show?}


%We shall see Section~\ref{sec:simulations} that the early combination of scores make it difficult to interpret whether improvements (deterioration) are due to more (less) understandable or more (less) topical documents being retrieved.

%While UBIRE framework can be successfully used to compare results across systems, as intended by evaluation campaigns such as CLEF eHealth, it is not trivial to identify how the retrieval performance of different dimensions affect the final score outputted by metrics of this framework.
%We shall see Section~\ref{sec:simulations} that the early combination of scores make it difficult to interpret whether improvements (deterioration) are due to more (less) understandable or more (less) topical documents being retrieved.

%What we propose is an alternative framework based on a later combination of intermediary results through a harmonic mean function.
%We describe our framework in Section~\ref{sec:extension} and, in order to fairly compare our framework to UBIRE, we modify the same RBP metric, resulting in a metric called $H_{RBP}$.
%
%Finally, we present our discussion and conclusions in Section~\ref{sec:eval_summary}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


