
\section{Evaluation Metrics of Understandability in Search Engines}
\label{chp:evaluation_metrics}

One of the first notions of relevance was straightforward: a document is relevant to a query if the topic of the information retrieved matches the topic of the request.
This was called \textit{topical relevance} by Eisenberg and Schamber~\cite{eisenberg88} and measured whether a document was ``on the topic'' or ``on the subject''.
Due to its simplicity and clear definition~\cite{borlund03}, this notion was adopted by most of the modern evaluation tasks, such as TREC\footnote{\url{http://trec.nist.gov}}, CLEF\footnote{\url{http://www.clef-initiative.eu/}} and NTCIR\footnote{\url{http://http://research.nii.ac.jp/ntcir/index-en.html}}, which heavily rely on evaluation metrics that exclusively measure the topical relevance of documents~\cite{voorhees05}.
Recently extensions were made based on notions of novelty and diversity (e.g.,~\cite{clarke09}), but these new metrics still mainly consider relevance with respect to document topicality.

However, document relevance cannot be attributed to just one factor such as topicality, instead, it is multidimensional and situational~\cite{borlund03}.
Early research~\cite{saracevic75,swanson86,harter92}, reviewed by Borlund divided relevance into two classes: (1) objective or system-based relevance and (2) subjective or human based relevance~\cite{borlund03}. 
This first class of relevance exclusively refers to the document ``aboutness'' aforementioned and it is context-free, while the second one refers to the subjective factors in both user and documents and it is context dependent.
A large body of subsequent research was dedicated to identifying the subjective aspects of relevance.
Park, for example, identified individual's subject knowledge, professional training, and educational background as a user-based influential factor, while scarcity, availability, timeliness, and scope as document-based factors~\cite{park93}.
Schamber published a compiling and non-exhaustive list of 80 relevance criteria suggested in the literature~\cite{schamber94}.
%
In the consumer health search domain, William Hersh highlighted two important factors to be considered by modern search engines~\cite{hersh08}: understandability and information trustworthiness. 
While the frameworks described in this paper are broad enough to be used with any multidimensional evaluation, we use as example here the consumer health search scenario as early approaches have been developed in this context.

Early efforts to integrate understandability into information retrieval evaluation was made by Zuccon and Koopman~\cite{zuccon14} and enhanced later by Zuccon~\cite{zuccon16}.
They refer to their framework as \textit{understandability-biased IR evaluation framework} (UBIRE).
Under UBIRE, metrics such as $uRBP$ and $uRBPgr$ were built upon the Rank Biased Precision metric (RBP)~\cite{moffat08}. These metrics are used in recent evaluation campaigns in the context of CLEF eHealth to combine understandability of documents with topical relevance (and more recently they included a dimension for document trustworthiness~\cite{clef17}.
$uRBP$, short for understandability-biased RBP and its graded version, $uRBPgr$, merge the relevance and understandability scores of a document into a unique score following the gain-discount framework by Carterette~\cite{carterette11}. We shall review UBIRE framework in Section~\ref{sec:understandability_metrics}. 

While UBIRE framework can be successfully used to compare results across systems, as intended by evaluation campaigns such as CLEF eHealth, it is not trivial to identify how the retrieval performance of different dimensions affect the final score outputted by metrics of this framework.
We shall see Section~\ref{sec:simulations} that the early combination of scores make it difficult to interpret whether improvements (deterioration) are due to more (less) understandable or more (less) topical documents being retrieved.

What we propose is an alternative framework based on a later combination of intermediary results through a harmonic mean function.
We describe our framework in Section~\ref{sec:extension} and, in order to fairly compare our framework to UBIRE, we modify the same RBP metric, resulting in a metric called $H_{RBP}$.

Finally, we present our discussion and conclusions in Section~\ref{sec:eval_summary}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


