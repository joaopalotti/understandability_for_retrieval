
\input{tables/tab_kendall}


\section{Rank Correlations} %across Systems and Measures}
\label{sec:clef}
Next, we aim to compare the behaviours of $H$ and UBIRE using real data. For this, we use the systems participating to the CLEF eHealth IR Lab evaluations in 2015 and 2016~\cite{clefIR15,clefIR16}. In both these evaluation challenges, systems were officially evaluated using $uRBP$ -- we further evaluate each system using $H$ and study the correlation among system rankings obtained using RBP (thus considering topicality only), $uRBP$ (UBIRE), and our proposed $RBP_u$ (thus considering only understandability) and $H_{RBP}$. The investigation of correlations among systems rankings is a common approach to compare and understand relative behaviour of evaluation measures~\cite{}. 

Specifically, we studied a setting where understandability was binary, akin to topicality, which also was considered as binary. For topicality, this was achieved using the common gain function for RBP that only models binary relevance: graded relevance labels were conflated to binary such that highly relevant and relevant assessments were mapped to relevant, and the rest to irrelevant. For understandability, the binarisation of the assessments was dependant on the year of the challenge. For 2015, understandability assessments were made on a 4-point scale (very easy, easy, hard and very hard)~\cite{clefIR15}: we made this binary by assuming that a document marked as very easy and easy was understandable, while we marked the remaining as not-understandable. For 2016, understandability assessments were made on a integer scale ranging from 0 (very easy) to 100 (very hard)~\cite{clefIR16}: we made this binary by assuming that documents with an assessment lower than or equal to 40 were understandable, while we marked the remaining as not-understandable. 


%In addition to the synthetic experiments studied in Section~\ref{sec:simulations}, we aim to understand how the proposed $H$ framework correlates with metrics from the UBIRE framework.
%For that, we use the participating systems in CLEF eHealth campaigns of 2015 and 2016~\cite{clefIR15,clefIR16}, in which systems were officially evaluated on their $uRBP$ score.
%Our goal is to understand the correlation among the regular RBP, $uRBP$, and our proposed $RBP_u$ and $H_{RBP}$.

%It is necessary to define $RBP_u(\rho)$ for both CLEF eHealth 2015 and 2016 in order to evaluate systems using the H framework. 
%Understandability in 2015 was collected with 4 labels (very easy, easy, hard and very hard)~\cite{clefIR15}: we assume that a document labelled as very easy and easy are relevant documents in the understandability dimension, while the others are non-relevant.
%In 2016, understandability labels were collected as integers ranging from 0 (very easy) to 100 (very hard)~\cite{clefIR16}. Based on the distribution of understandability scores, we make the assumption that documents with a label lower than 40 are relevant for the understandability dimension, while the others are non-relevant.

Table~\ref{tab:kendall} shows the Kendall-$\tau$ rank correlations of system in both CLEF 2015 and 2016 according to RBP, $uRBP$, $RBP_u$ and $H_{RBP}$. Rank correlation between RBP and $uRBP$ was very high for both 2015 and 2016 data. This emphasises the tight relation between RBP and $uRBP$. On the other hand, $H_{RBP}$ exhibited the strongest rank correlations with $RBP_u$, while the correlations between $RBP_u$ and RBP or $uRBP$ are marginal. In addition, we found that $H_{RBP}$ strongly correlates with RBP, but not as strongly as  $uRBP$ does. Finally, $H_{RBP}$ and $uRBP$ showed generally high correlations among themselves, highlighting that the two measures provide similar evaluations of system effectiveness, but that $H_{RBP}$ has the advantage that the trade-off between topicality and understandability can be clearly identified and studied. 

%In both in 2015 and 2016, the correlation between RBP and $uRBP$ is very high (0.90 and 0.94). This emphasizes once more that RBP and $uRBP$ are tightly bounded.
%In turn, the metric that strongest correlates with $RBP_u$ is $H_{RBP}$, and it only weakly correlates with RBP and $uRBP$.
%Finally, $H_{RBP}$ correlation with RBP is high (0.84 in 2015 and 0.85 in 2016), but not as strong as the correlation between RBP and $uRBP$.
%The correlation between $H_{RBP}$ and $uRBP$ shows that $H_{RBP}$ is as capable as $uRBP$ to rank systems.

%\mytodo{Something else?}
 
